{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-28 20:19:17,493\tINFO resource_spec.py:205 -- Starting Ray with 4.64 GiB memory available for workers and up to 2.79 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import ray\n",
    "import model\n",
    "import random,time\n",
    "from time import sleep\n",
    "import copy \n",
    "\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init(memory=5000000000,object_store_memory=3000000000)\n",
    "\n",
    "# @ray.remote(num_gpus=1)\n",
    "@ray.remote\n",
    "class ParameterServer():\n",
    "    def __init__(self,learning_rate,num_workers,stalness_limit):\n",
    "        self.net = model.SimpleCNN(learning_rate=learning_rate)\n",
    "        self.stalness_table = [0] * num_workers\n",
    "        self.stalness_limit = stalness_limit \n",
    "        self.global_step = 0\n",
    "        self.eva_model = model.SimpleCNN()\n",
    "        self.mnist = model.download_mnist_retry()\n",
    "        \n",
    "    def apply_gradients(self, gradients, wk_idx):\n",
    "        self.net.apply_gradients(gradients)\n",
    "        self.stalness_table[wk_idx] += 1\n",
    "        self.global_step += 1\n",
    "        if self.global_step % 100 == 0:\n",
    "            print(\"global_step: \",self.global_step,\" and prepare evaluate\")\n",
    "            self.evaluate()\n",
    "        \n",
    "    def pull_weights(self):\n",
    "        # return value of weights\n",
    "        return self.net.get_weights()\n",
    "    \n",
    "    def check_stalness(self,wk_idx):\n",
    "        min_iter = min(self.stalness_table)\n",
    "        return self.stalness_table[wk_idx] - min_iter < self.stalness_limit\n",
    "        \n",
    "    def get_stalness(self):\n",
    "        return min(self.stalness_table)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        cur_wei = self.net.get_weights()\n",
    "        self.eva_model.set_weights(cur_wei[0],cur_wei[1])\n",
    "        test_xs, test_ys = self.mnist.test.next_batch(1000)\n",
    "        accuracy = self.eva_model.compute_accuracy(test_xs, test_ys)\n",
    "        print(\"Iteration {}: accuracy is {}\".format(self.global_step, accuracy))\n",
    "        \n",
    "@ray.remote\n",
    "def worker_task(ps,worker_index,stale_limit,batch_size=50):\n",
    "    mnist = model.download_mnist_retry(seed=worker_index)\n",
    "    # Initialize the model.\n",
    "    net = model.SimpleCNN()\n",
    "    keys = net.get_weights()[0]\n",
    "    local_step = 0\n",
    "    \n",
    "    while True:\n",
    "        while(local_step - ray.get(ps.get_stalness.remote()) > stale_limit):\n",
    "            print(worker_index,\" works too fast\")\n",
    "            sleep(1)\n",
    "            \n",
    "        # Get the current weights from the parameter server.\n",
    "        init_wei = ray.get(ps.pull_weights.remote())\n",
    "        net.set_weights(init_wei[0], init_wei[1])\n",
    "\n",
    "        # Compute an update and push it to the parameter server.\n",
    "        xs, ys = mnist.train.next_batch(batch_size)\n",
    "        gradients = net.compute_gradients(xs, ys)\n",
    "        local_step += 1\n",
    "        ps.apply_gradients.remote(gradients,worker_index)\n",
    "#         print(worker_index,\"has finished update\")\n",
    "\n",
    "# @ray.remote\n",
    "# class Worker():\n",
    "#     def __init__(self,worker_index,init_wei, batch_size=50):\n",
    "#         self.worker_index = worker_index\n",
    "#         self.batch_size = batch_size\n",
    "#         self.mnist = model.download_mnist_retry(seed=worker_index)\n",
    "#         # the init weight is randomly assigned\n",
    "#         self.net = model.SimpleCNN()\n",
    "#         self.net.set_weights(init_wei[0],init_wei[1])\n",
    "#         self.iter_count = 0\n",
    "\n",
    "#     def iterater(self,ps_handler):\n",
    "#         while not ray.get(ps_handler.check_stalness.remote(self.worker_index)):\n",
    "# #             print(self.worker_index,\" works too fast\")\n",
    "#             sleep(1)\n",
    "#         ps_handler.apply_gradients.remote(self._compute_gradients(),self.worker_index)\n",
    "#         if self.iter_count % 5 == 0:\n",
    "#             self.sync_wei(ps_handler)\n",
    "    \n",
    "#     def _compute_gradients(self):\n",
    "#         # simulate network delay\n",
    "# #         sleep(random.randint(0,3))\n",
    "# #         print(self.worker_index,\"is going to compute gradients\")\n",
    "#         xs,ys=self.mnist.train.next_batch(self.batch_size)\n",
    "#         gradients = self.net.compute_gradients(xs,ys)\n",
    "#         self.net.apply_gradients(gradients)\n",
    "#         self.iter_count += 1 \n",
    "#         return gradients\n",
    "    \n",
    "#     def set_weights(self,keys,weights):\n",
    "#         self.net.set_weights(keys,weights)\n",
    "    \n",
    "#     def get_weights(self):\n",
    "#         return self.net.get_weights()\n",
    "    \n",
    "#     def sync_wei(self,ps_handler):\n",
    "#          # sync wei from ps \n",
    "#         sync_wei = ray.get(ps_handler.pull_weights.remote())\n",
    "#         self.net.set_weights(sync_wei[0],sync_wei[1])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m WARNING:tensorflow:From /userhome/34/gyu/git-repo/ray/examples/parameter_server/model.py:22: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m WARNING:tensorflow:From /userhome/34/gyu/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m Please write your own downloading logic.\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m WARNING:tensorflow:From /userhome/34/gyu/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m Please use tf.data to implement this functionality.\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m WARNING:tensorflow:From /userhome/34/gyu/git-repo/ray/examples/parameter_server/model.py:22: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m WARNING:tensorflow:From /userhome/34/gyu/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m Please write your own downloading logic.\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m WARNING:tensorflow:From /userhome/34/gyu/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m Please use tf.data to implement this functionality.\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m WARNING:tensorflow:From /userhome/34/gyu/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m Please use tf.data to implement this functionality.\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m WARNING:tensorflow:From /userhome/34/gyu/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m Please use tf.one_hot on tensors.\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m WARNING:tensorflow:From /userhome/34/gyu/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m WARNING:tensorflow:From /userhome/34/gyu/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m Please use tf.data to implement this functionality.\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m WARNING:tensorflow:From /userhome/34/gyu/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m Please use tf.one_hot on tensors.\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m WARNING:tensorflow:From /userhome/34/gyu/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m WARNING:tensorflow:From /userhome/34/gyu/git-repo/ray/examples/parameter_server/model.py:43: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m Future major versions of TensorFlow will allow gradients to flow\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m into the labels input on backprop by default.\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m WARNING:tensorflow:From /userhome/34/gyu/git-repo/ray/examples/parameter_server/model.py:43: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Future major versions of TensorFlow will allow gradients to flow\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m into the labels input on backprop by default.\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m WARNING:tensorflow:From /userhome/34/gyu/git-repo/ray/examples/parameter_server/model.py:22: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m WARNING:tensorflow:From /userhome/34/gyu/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m Please write your own downloading logic.\n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m WARNING:tensorflow:From /userhome/34/gyu/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m Please use tf.data to implement this functionality.\n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m Extracting MNIST_data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m WARNING:tensorflow:From /userhome/34/gyu/git-repo/ray/examples/parameter_server/model.py:43: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m Future major versions of TensorFlow will allow gradients to flow\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m into the labels input on backprop by default.\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m WARNING:tensorflow:From /userhome/34/gyu/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m Please use tf.data to implement this functionality.\n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m WARNING:tensorflow:From /userhome/34/gyu/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m Please use tf.one_hot on tensors.\n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m WARNING:tensorflow:From /userhome/34/gyu/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m WARNING:tensorflow:From /userhome/34/gyu/git-repo/ray/examples/parameter_server/model.py:43: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m Future major versions of TensorFlow will allow gradients to flow\n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m into the labels input on backprop by default.\n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m WARNING:tensorflow:From /userhome/34/gyu/git-repo/ray/examples/parameter_server/model.py:22: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m WARNING:tensorflow:From /userhome/34/gyu/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Please write your own downloading logic.\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m WARNING:tensorflow:From /userhome/34/gyu/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Please use tf.data to implement this functionality.\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m WARNING:tensorflow:From /userhome/34/gyu/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Please use tf.data to implement this functionality.\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m WARNING:tensorflow:From /userhome/34/gyu/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Please use tf.one_hot on tensors.\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m WARNING:tensorflow:From /userhome/34/gyu/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  100  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 100: accuracy is 0.8119999766349792\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  200  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 200: accuracy is 0.9010000228881836\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  300  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 300: accuracy is 0.9309999942779541\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  400  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 400: accuracy is 0.9549999833106995\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  500  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 500: accuracy is 0.9660000205039978\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  600  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 600: accuracy is 0.9490000009536743\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  700  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 700: accuracy is 0.9700000286102295\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m 0  works too fast\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  800  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 800: accuracy is 0.9419999718666077\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  900  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 900: accuracy is 0.9620000123977661\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  1000  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 1000: accuracy is 0.9819999933242798\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  1100  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 1100: accuracy is 0.972000002861023\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  1200  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 1200: accuracy is 0.9779999852180481\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m 0  works too fast\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  1300  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 1300: accuracy is 0.9729999899864197\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  1400  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 1400: accuracy is 0.9779999852180481\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  1500  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 1500: accuracy is 0.9670000076293945\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  1600  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 1600: accuracy is 0.968999981880188\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  1700  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 1700: accuracy is 0.9789999723434448\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  1800  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 1800: accuracy is 0.9810000061988831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  1900  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 1900: accuracy is 0.9810000061988831\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  2000  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 2000: accuracy is 0.9800000190734863\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  2100  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 2100: accuracy is 0.9800000190734863\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  2200  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 2200: accuracy is 0.9890000224113464\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m 0  works too fast\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m 2  works too fast\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  2300  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 2300: accuracy is 0.9789999723434448\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  2400  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 2400: accuracy is 0.9789999723434448\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  2500  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 2500: accuracy is 0.9779999852180481\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  2600  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 2600: accuracy is 0.9900000095367432\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  2700  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 2700: accuracy is 0.9890000224113464\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  2800  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 2800: accuracy is 0.9850000143051147\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  2900  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 2900: accuracy is 0.9860000014305115\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  3000  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 3000: accuracy is 0.9869999885559082\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  3100  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 3100: accuracy is 0.9789999723434448\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m 2  works too fast\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m 0  works too fast\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  3200  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 3200: accuracy is 0.9900000095367432\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  3300  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 3300: accuracy is 0.9850000143051147\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m 0  works too fast\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  3400  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 3400: accuracy is 0.9909999966621399\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  3500  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 3500: accuracy is 0.9860000014305115\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  3600  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 3600: accuracy is 0.9900000095367432\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m 0  works too fast\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  3700  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 3700: accuracy is 0.9919999837875366\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  3800  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 3800: accuracy is 0.9779999852180481\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  3900  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 3900: accuracy is 0.9869999885559082\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  4000  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 4000: accuracy is 0.9860000014305115\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  4100  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 4100: accuracy is 0.9890000224113464\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  4200  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 4200: accuracy is 0.9850000143051147\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m 2  works too fast\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m 0  works too fast\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  4300  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 4300: accuracy is 0.9909999966621399\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  4400  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 4400: accuracy is 0.984000027179718\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m 0  works too fast\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  4500  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 4500: accuracy is 0.9760000109672546\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  4600  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 4600: accuracy is 0.9860000014305115\n",
      "\u001b[2m\u001b[36m(pid=13207)\u001b[0m 2  works too fast\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m 0  works too fast\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  4700  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 4700: accuracy is 0.9879999756813049\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  4800  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 4800: accuracy is 0.9900000095367432\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  4900  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 4900: accuracy is 0.9940000176429749\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  5000  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 5000: accuracy is 0.9900000095367432\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  5100  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 5100: accuracy is 0.984000027179718\n",
      "\u001b[2m\u001b[36m(pid=13201)\u001b[0m 0  works too fast\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  5200  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 5200: accuracy is 0.9900000095367432\n",
      "\u001b[2m\u001b[36m(pid=13200)\u001b[0m 1  works too fast\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  5300  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 5300: accuracy is 0.9860000014305115\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  5400  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 5400: accuracy is 0.9900000095367432\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  5500  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 5500: accuracy is 0.9950000047683716\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m global_step:  5600  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=13202)\u001b[0m Iteration 5600: accuracy is 0.9860000014305115\n"
     ]
    }
   ],
   "source": [
    "num_worker = 3\n",
    "stalness_table = [0] * num_worker\n",
    "stalness_limit = 4\n",
    "\n",
    "ps = ParameterServer.remote(1e-3,num_worker,stalness_limit)\n",
    "# init_wei = ray.get(ps.pull_weights.remote())\n",
    "# workers = [Worker.remote(index,init_wei) for index in range(num_worker)]\n",
    "worker_tasks = [worker_task.remote(ps,i,stalness_limit) for i in range(num_worker)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _ in range(2000):\n",
    "#     [worker.iterater.remote(ps) for idx,worker in enumerate(workers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_grad_apply(worker,worker_index,ps):\n",
    "#     sleep(random.randint(0, 3))\n",
    "#     grad = ray.get(worker.compute_gradients.remote())\n",
    "#     ps.apply_gradients.remote(grad)\n",
    "#     stalness_table[worker_index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_stalness(stalness_table,worker_index,stalness_limit):\n",
    "#     max_iter = max(stalness_table.values())\n",
    "#     return max_iter - stalness_table[worker_index] < stalness_limit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def worker_iter(idx,worker):\n",
    "#     print(idx,\"starts to update\")\n",
    "#     while not check_stalness(stalness_table,idx,stalness_limit):\n",
    "#         print(\"meet stalness upper limit\")\n",
    "#         sleep(1)\n",
    "#     compute_grad_apply(worker,idx,ps)\n",
    "#     print(idx,\"ends update\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for(_ in range(100)):\n",
    "#     for wk_idx in len(workers):\n",
    "#         while not check_stalness(stalness_table,wk_idx,stalness_limit=10):\n",
    "#             print(\"meet stalness limit\")\n",
    "#             sleep(1)\n",
    "#         worker.compute_gradients.remote()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 检查 并行？ \n",
    "# from time import sleep\n",
    "# import random\n",
    "\n",
    "# @ray.remote\n",
    "# class test():\n",
    "#     def __init__(self):\n",
    "#         self.count = 0\n",
    "#     def sleep_inc(self,idx):\n",
    "#         sleep(random.randint(0,3))\n",
    "#         self.count += 1\n",
    "#         print(idx,\"has incresed\")\n",
    "#         self.help_fun()\n",
    "#         return self.count\n",
    "    \n",
    "#     def help_fun(self):\n",
    "#         print('helping')\n",
    "    \n",
    "# tests = [test.remote() for _ in range(5)]\n",
    "# [t.sleep_inc.remote(idx) for idx,t in enumerate(tests)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel_for_tensorflow",
   "language": "python",
   "name": "kernel_for_tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
