{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# pytorch optimizer 让动量参与计算，以及手动修改lr\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import ray\n",
    "import resnet.models as models\n",
    "import random,time\n",
    "from time import sleep\n",
    "import copy \n",
    "import datetime\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from filelock import FileLock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-14-3dfd6e541c5a>, line 130)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-3dfd6e541c5a>\"\u001b[0;36m, line \u001b[0;32m130\u001b[0m\n\u001b[0;31m    def pull_weights(self):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def generate_train_loader(batch_size,kwargs):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10('./data.cifar10', train=True, download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.Pad(4),\n",
    "            transforms.RandomCrop(32),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    return train_loader\n",
    "\n",
    "def generate_test_loader(test_batch_size):\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10('./data.cifar10', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        ])),batch_size=test_batch_size, shuffle=True)\n",
    "    return test_loader\n",
    "\n",
    "def _get_params(model):\n",
    "    bns = {}\n",
    "    non_bns = {}\n",
    "    param_count = 0.\n",
    "    bn_param_count = 0.\n",
    "    for name,param in model.named_parameters():\n",
    "        param_count += len(param)\n",
    "        if 'bn' in name:\n",
    "            bns[name] = param\n",
    "            bn_param_count += len(param)\n",
    "        else:\n",
    "            non_bns[name] = param\n",
    "#     print(\"bn params occupies: \",bn_param_count/param_count)\n",
    "    return bns,non_bns\n",
    "\n",
    "@ray.remote\n",
    "class ParameterServer():\n",
    "    def __init__(self,args,test_loader):\n",
    "        self.model = models.__dict__[\"resnet\"](dataset=\"cifar10\",depth=args.depth)\n",
    "        self.stalness_table = [0] * args.num_workers\n",
    "        self.stalness_limit = args.stalness_limit \n",
    "        self.global_step = 0\n",
    "        self.lr = args.lr\n",
    "        self.args = args\n",
    "        self.eva_model = models.__dict__[\"resnet\"](dataset=\"cifar10\",depth=args.depth)\n",
    "        self.optimizer = optim.SGD(self.model.parameters(),\n",
    "                          lr=args.lr,\n",
    "                          momentum=args.momentum,\n",
    "                          weight_decay=args.weight_decay)\n",
    "        self.test_loader = test_loader\n",
    "        self.model.cpu()\n",
    "        self.eva_model.cpu()\n",
    "        self.ps_writer = SummaryWriter(os.path.join(os.getcwd(),(args.tb_path+'/ps')))\n",
    "        self.save_path = args.save\n",
    "        self.num_workers = args.num_workers\n",
    "        \n",
    "        # get point to all non_bns parameters\n",
    "        self.non_bns = [param.data for name,param in self.model.named_parameters() if 'bn' not in name]\n",
    "        self.bns = [param.data for name,param in self.model.named_parameters() if 'bn' in name]\n",
    "        self.bns_sync = [None] * args.num_workers\n",
    "        \n",
    "        if args.resume:\n",
    "            if os.path.isfile(args.resume):\n",
    "                print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "                checkpoint = torch.load(args.resume)\n",
    "                self.global_step = checkpoint['global_step']\n",
    "                self.model.load_state_dict(checkpoint['state_dict'])\n",
    "                self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "                self.stalness_table = [self.global_step/args.num_workers] * args.num_workers\n",
    "                print(\"=> loaded checkpoint '{}' (global step: {})\".format(args.resume, checkpoint['global_step']))                \n",
    "                if 'epoch' in checkpoint: print(\"epoch: {}\".format(checkpoint['epoch']))\n",
    "            else:\n",
    "                print(\"=> no checkpoint found at '{}'\".format(args.resume))                    \n",
    "                \n",
    "    def apply_gradients(self, iter_diff, wk_idx, epoch):\n",
    "        if args.debug: print(\"applying gradients from the \",wk_idx, \" worker\")\n",
    "        \n",
    "        # updata all params\n",
    "        for idx, p in enumerate(self.model.parameters()):\n",
    "            p.data -= iter_diff[idx]\n",
    "\n",
    "#         if wk_idx == 0:\n",
    "#             # updata all params\n",
    "#             for idx, p in enumerate(self.model.parameters()):\n",
    "#                 p.data -= iter_diff[idx]\n",
    "#         else:\n",
    "#             # only update non_bns params\n",
    "#             for idx, tensor in enumerate(self.non_bns):\n",
    "#                 tensor -= iter_diff[idx]\n",
    "        \n",
    "#         # only update non_bns params\n",
    "#         for idx, tensor in enumerate(self.non_bns):\n",
    "#             tensor -= iter_diff[idx]\n",
    "        \n",
    "        self.stalness_table[wk_idx] += 1\n",
    "        self.global_step += 1\n",
    "        if args.debug: print(\"finished applying gradients from the \",wk_idx, \" worker\")\n",
    "        if self.global_step % 1000 == 0:\n",
    "#             print(\"global_step: \",self.global_step,\" and prepare evaluate\")\n",
    "#             self.evaluate()\n",
    "            self.save_ckpt({\n",
    "                'epoch':epoch,\n",
    "                'global_step':self.global_step,\n",
    "                'state_dict':self.model.state_dict(),\n",
    "                'optimizer':self.optimizer.state_dict()\n",
    "            },filepath=os.path.join(os.getcwd(),self.save_path))\n",
    "    \n",
    "    \n",
    "    def apply_gradients_non_bns(self,iter_diff,wk_idx,epoch):\n",
    "        for i in range(len(self.non_bns)):\n",
    "            self.non_bns[i] -= iter_diff[i]\n",
    "#         print(wk_idx,\"finished updating non bns on ps\")\n",
    "        self.stalness_table[wk_idx] += 1\n",
    "        self.global_step += 1\n",
    "\n",
    "        \n",
    "    def apply_gradients_bns(self,iter_diff,wk_idx,epoch):\n",
    "        for idx, tensor in enumerate(self.bns):\n",
    "#             if idx == len(iter_diff) / 2:\n",
    "#                 print(wk_idx,\"is in the middle of updating bns on ps\")\n",
    "            tensor -= iter_diff[idx]\n",
    "#         print(wk_idx,\"finished updating non bns on ps\")\n",
    "#         self.stalness_table[wk_idx] += 1\n",
    "#         self.global_step += 1\n",
    "\n",
    "    def apply_gradients_partical_bns(self, iter_diff, wk_idx, epoch):\n",
    "        if wk_idx == 0 :\n",
    "            for i in range(0,len(self.bns)/self.num_workers):\n",
    "                self.bns[i] -= iter_diff[i]\n",
    "        elif wk_idx == self.num_workers:\n",
    "            for i in range(len(self.bns) * wk_idx / self.num_workers + 1 , len(self.bns)):\n",
    "                self.bns[i] -= iter_diff[i]\n",
    "        else:\n",
    "            for i in range(len(self.bns) * wk_idx / self.num_workers + 1 , len(self.bns) * (wk_idx + 1) / self.num_workers):\n",
    "                self.bns[i] -= iter_diff[i]\n",
    "        \n",
    "    def pull_weights(self):\n",
    "        return self.model.state_dict()\n",
    "    \n",
    "    def pull_non_bn_weights(self):\n",
    "        self.non_bns = [param.data for name,param in self.model.named_parameters() if 'bn' not in name]\n",
    "        return copy.deepcopy(self.non_bns)\n",
    "    \n",
    "    def pull_bn_weights(self):\n",
    "        self.bns = [param.data for name,param in self.model.named_parameters() if 'bn' in name]\n",
    "        return copy.deepcopy(self.bns)\n",
    "    \n",
    "    def get_optim(self):\n",
    "        return self.optimizer\n",
    "    \n",
    "    def pull_optimizer_state(self):\n",
    "        return self.optimizer.state_dict()\n",
    "\n",
    "    def check_stalness(self,wk_idx):\n",
    "        min_iter = min(self.stalness_table)\n",
    "        return self.stalness_table[wk_idx] - min_iter < self.stalness_limit\n",
    "        \n",
    "    def get_stalness(self):\n",
    "        return min(self.stalness_table)\n",
    "    \n",
    "    def get_stalness_table(self):\n",
    "        return self.stalness_table\n",
    "    \n",
    "    def get_global_step(self):\n",
    "        return self.global_step\n",
    "    \n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def save_ckpt(self,state,filepath):\n",
    "        torch.save(state,os.path.join(filepath,'checkpoint.pth.tar'))\n",
    "    \n",
    "    def get_bns_ready(self):\n",
    "        return any(self.bns_sync) == False\n",
    "        \n",
    "    def aggregate_bns(self,wk_bns,worker_index):\n",
    "        self.bns_sync[worker_index] = wk_bns\n",
    "        if all(self.bns_sync):\n",
    "            for i in range(len(self.bns)):\n",
    "                tmp = copy.deepcopy(self.bns_sync[0][i])\n",
    "                for j in range(1, self.num_workers):\n",
    "                    tmp += self.bns_sync[j][i]\n",
    "                self.bns[i] = tmp / self.num_workers    \n",
    "            self.bns_sync = [None] * self.num_workers\n",
    "        return self.bns_sync\n",
    "    \n",
    "    def evaluate(self):\n",
    "        print(\"going to evaluate\")\n",
    "        test_loss = 0.\n",
    "        correct = 0.\n",
    "        print(\"pulled weights\")\n",
    "        self.eva_model.load_state_dict(copy.deepcopy(self.model.state_dict()))\n",
    "        print(\"loaded weights\")\n",
    "        print(\"length of the test_loader dataset is : \",len(self.test_loader.dataset))\n",
    "        self.eva_model.eval()\n",
    "        count = 0\n",
    "        for data,target in self.test_loader:\n",
    "            count += 1\n",
    "            if count % 20 == 0: print(\"in eval, the batch is: \",count)\n",
    "            data, target = Variable(data,volatile=True),Variable(target)\n",
    "            output = self.eva_model(data)\n",
    "            batch_loss = F.cross_entropy(output, target, size_average=False).data\n",
    "            test_loss += batch_loss\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        len_testset = len(self.test_loader.dataset)\n",
    "        test_loss /= len_testset \n",
    "        accuracy = correct / len_testset\n",
    "        # log \n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
    "        test_loss, correct, len_testset,accuracy))\n",
    "\n",
    "        self.ps_writer.add_scalar('Accuracy/eval', accuracy, self.global_step)\n",
    "        self.ps_writer.add_scalar('Loss/eval',test_loss , self.global_step)\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_gpus=1)\n",
    "def worker_task(args,ps,worker_index, train_loader):\n",
    "    # Initialize the model.\n",
    "#     if args.debug: print(worker_index, \" worker is going to sleep \",worker_index*5000)\n",
    "#     time.sleep(worker_index * 5000)\n",
    "    \n",
    "    model = models.__dict__[\"resnet\"](dataset=\"cifar10\",depth=args.depth)\n",
    "    local_step = 0\n",
    "    optimizer = optim.SGD(model.parameters(),\n",
    "                          lr=args.lr,\n",
    "                          momentum=args.momentum,\n",
    "                          weight_decay=args.weight_decay)\n",
    "    wk_non_bns = [param.data for name,param in model.named_parameters() if 'bn' not in name]\n",
    "    wk_bns = [param.data for name,param in model.named_parameters() if 'bn' in name]\n",
    "    \n",
    "    if args.cuda:\n",
    "        starttime = datetime.datetime.now()\n",
    "        model.cuda()\n",
    "        endtime = datetime.datetime.now()\n",
    "        time_cost = (endtime - starttime).seconds\n",
    "        if args.debug: print(\"move model to gpu takes: \", time_cost, \"seconds\")\n",
    "    if args.resume:\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        local_step = checkpoint['global_step'] / args.num_workers\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        if 'epoch' in checkpoint:\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "\n",
    "    wk_writer = SummaryWriter(os.path.join(os.getcwd(),args.tb_path,('wk_'+str(worker_index))))\n",
    "    print(\"worker #\",worker_index,\" is online\")\n",
    "    \n",
    "    # all workers owns the same init values\n",
    "    init_wei = ray.get(ps.pull_weights.remote())\n",
    "    model.load_state_dict(init_wei)\n",
    "    sync_bns_flag = False\n",
    "    \n",
    "    \n",
    "    for epoch in range(args.start_epoch,args.epochs):\n",
    "        avg_loss = 0.\n",
    "        train_correct = 0.\n",
    "        for batch_idx,(data,target) in enumerate(train_loader):\n",
    "            if args.cuda:\n",
    "                starttime = datetime.datetime.now()\n",
    "                data,target = data.cuda(),target.cuda()\n",
    "                mid = datetime.datetime.now()\n",
    "                if args.debug: print(\"move data to gpu takes: \", (mid - starttime).seconds, \"seconds\")\n",
    "                model.cuda()\n",
    "                endtime = datetime.datetime.now()\n",
    "                time_cost = (endtime - starttime).seconds\n",
    "                if args.debug: print(\"move model to gpu takes: \", time_cost, \"seconds\")\n",
    "                \n",
    "            while(local_step - ray.get(ps.get_stalness.remote()) > args.stalness_limit):\n",
    "                sleep(1)\n",
    "            \n",
    "\n",
    "            # Get all weights from the parameter server.\n",
    "            if args.debug: print(\"the \",worker_index,\" pulls wei from ps.\")\n",
    "            init_wei = ray.get(ps.pull_weights.remote())\n",
    "            model.load_state_dict(init_wei)\n",
    "            if args.debug: print(\"the \",worker_index,\" loaded the latest wei from ps.\")\n",
    "                \n",
    "#             # Get only non-bn weights from the parameter server.\n",
    "#             ps_non_bns = ray.get(ps.pull_non_bn_weights.remote())\n",
    "#             print(worker_index,\"pulled non bns from ps\")\n",
    "#             assert len(ps_non_bns) == len(wk_non_bns)\n",
    "#             for i in range(len(ps_non_bns)):\n",
    "#                 if i == len(ps_non_bns) / 2:\n",
    "#                     print(worker_index,\"is in the middle of updating non bns\")\n",
    "#                 wk_non_bns[i] = ps_non_bns[i]\n",
    "#             print(worker_index,\"updated non bns from ps and is going to pull bns \")\n",
    "            \n",
    "#             # Get only bn weights from the parameter server.\n",
    "#             ps_bns = ray.get(ps.pull_bn_weights.remote())\n",
    "#             print(worker_index,\"pulled bns from ps\")\n",
    "#             assert len(ps_bns) == len(wk_bns)\n",
    "#             for i in range(len(ps_bns)):\n",
    "#                 if i == len(ps_bns) / 2 :\n",
    "#                     print(worker_index, \"is in the middle of updating bns\")\n",
    "#                 wk_bns[i] = ps_bns[i]\n",
    "#             print(worker_index,\"updated bns from ps\")\n",
    "\n",
    "                \n",
    "            # Compute an update and push it to the parameter server.        \n",
    "            data, target = Variable(data), Variable(target)\n",
    "            optimizer.zero_grad()\n",
    "            if args.debug: print(worker_index,' is generating output')\n",
    "            output = model(data)\n",
    "            if args.debug: print(worker_index,' generated output done and going to calculate loss')\n",
    "            loss = F.cross_entropy(output,target)\n",
    "            avg_loss += loss\n",
    "            pred = output.data.max(1,keepdim=True)[1]\n",
    "            batch_correct = pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "            train_correct += batch_correct\n",
    "            if args.debug: print(worker_index,' calculated loss and going to bp')\n",
    "            loss.backward()\n",
    "            if args.debug: print(worker_index,' bp done')\n",
    "            starttime = datetime.datetime.now()\n",
    "            model.cpu()\n",
    "            endtime = datetime.datetime.now()\n",
    "            time_cost = (endtime - starttime).seconds\n",
    "            if args.debug: print(\"move model to cpu takes: \", time_cost, \"seconds\")\n",
    "            \n",
    "            \n",
    "            if worker_index == 0:\n",
    "                # calculate difference for this iteration\n",
    "                old_tensors = copy.deepcopy([p.data for p in model.parameters()])    \n",
    "                optimizer.step()\n",
    "                new_tensors = [p.data for p in model.parameters()]\n",
    "                iter_diff = [(old_tensor - new_tensor)/args.num_workers for (old_tensor, new_tensor) in zip(old_tensors,new_tensors)]\n",
    "                # print(\"passing sizes: \",sys.getsizeof(iter_diff))\n",
    "                ps.apply_gradients.remote(iter_diff,worker_index,epoch)\n",
    "            \n",
    "            else:\n",
    "                old_tensors_non_bns = copy.deepcopy([param.data for name,param in model.named_parameters() if 'bn' not in name])\n",
    "                optimizer.step()\n",
    "                wk_non_bns = [param.data for name,param in model.named_parameters() if 'bn' not in name]\n",
    "                iter_diff_non_bns = [(old_tensor_non_bns - new_tensor_non_bns)/args.num_workers for (old_tensor_non_bns, new_tensor_non_bns) in zip(old_tensors_non_bns,wk_non_bns)]\n",
    "                ps.apply_gradients_non_bns.remote(iter_diff_non_bns,worker_index,epoch)\n",
    "        \n",
    "\n",
    "                \n",
    "            \n",
    "#             if worker_index == 0:\n",
    "# #                 print(worker_index, \"passing all params\")\n",
    "#                 # calculate difference for this iteration\n",
    "#                 old_tensors = copy.deepcopy([p.data for p in model.parameters()])    \n",
    "#                 optimizer.step()\n",
    "#                 new_tensors = [p.data for p in model.parameters()]\n",
    "#                 iter_diff = [(old_tensor - new_tensor)/args.num_workers for (old_tensor, new_tensor) in zip(old_tensors,new_tensors)]\n",
    "#                 # print(\"passing sizes: \",sys.getsizeof(iter_diff))\n",
    "#                 ps.apply_gradients.remote(iter_diff,worker_index,epoch)\n",
    "#             else:\n",
    "# #                 print(worker_index, \"passing non bn params\")\n",
    "#                 #calculate only non-bn parameters difference \n",
    "#                 old_tensors = copy.deepcopy([param.data for name,param in model.named_parameters() if 'bn' not in name])\n",
    "#                 optimizer.step()\n",
    "#                 new_tensors = [param.data for name,param in model.named_parameters() if 'bn' not in name]\n",
    "#                 iter_diff = [(old_tensor - new_tensor)/args.num_workers for (old_tensor, new_tensor) in zip(old_tensors,new_tensors)]\n",
    "#                 # print(\"passing sizes: \",sys.getsizeof(iter_diff))\n",
    "#                 ps.apply_gradients.remote(iter_diff,worker_index,epoch)\n",
    "\n",
    "              # calculate parameters difference \n",
    "#             old_tensors_non_bns = copy.deepcopy([param.data for name,param in model.named_parameters() if 'bn' not in name])\n",
    "# #             old_tensors_bns = copy.deepcopy([param.data for name,param in model.named_parameters() if 'bn' in name])\n",
    "#             optimizer.step()\n",
    "#             new_tensors_non_bns = [param.data for name,param in model.named_parameters() if 'bn' not in name]\n",
    "# #             new_tensors_bns = [param.data for name,param in model.named_parameters() if 'bn' in name]\n",
    "            \n",
    "#             iter_diff_non_bns = [(old_tensor_non_bns - new_tensor_non_bns)/args.num_workers for (old_tensor_non_bns, new_tensor_non_bns) in zip(old_tensors_non_bns,new_tensors_non_bns)]\n",
    "# #             iter_diff_bns = [(old_tensor_bns - new_tensor_bns)/args.num_workers for (old_tensor_bns, new_tensor_bns) in zip(old_tensors_bns,new_tensors_bns)]\n",
    "#             # print(\"passing sizes: \",sys.getsizeof(iter_diff))\n",
    "#             ps.apply_gradients_non_bns.remote(iter_diff_non_bns,worker_index,epoch)\n",
    "#             print(worker_index, \"pushed non bns to ps and is going to push bns to ps\")\n",
    "#             ps.apply_gradients_bns.remote(iter_diff_bns,worker_index,epoch)\n",
    "#             print(worker_index, \"pushed bns to ps\")\n",
    "            \n",
    "            \n",
    "#             # aggregate and sync bn parameters    \n",
    "#             if sync_bns_flag:\n",
    "#                 print(\"SYNC BNS: worker #\",worker_index,\" is checking to pull bns from ps \")\n",
    "#                 if ray.get(ps.get_bns_ready.remote()):\n",
    "#                     print(\"SYNC BNS: worker #\",worker_index,\" is pulling bns from ps \")\n",
    "#                     ps_wei = ray.get(ps.pull_weights.remote())\n",
    "#                     model.load_state_dict(ps_wei)\n",
    "#                     sync_bns_flag = False\n",
    "#             if local_step % args.sync_bns == 0:\n",
    "#                 sync_bns_flag = True\n",
    "#                 model.cpu()\n",
    "#                 if args.debug: print(\"SYNC BNS: goint to aggregate bns\")\n",
    "#                 wk_bns = [param.data for name,param in model.named_parameters() if 'bn' in name]\n",
    "#                 bns_sync = ray.get(ps.aggregate_bns.remote(wk_bns,worker_index))\n",
    "#                 print(\"SYNC BNS: bns of worker #\",worker_index,\" have been pushed\")\n",
    "                \n",
    "#                 if any(bns_sync) == False: # when all workers have pushed their own bns parameters to ps\n",
    "#                     print(\"SYNC BNS: worker #\",worker_index,\" is last worker pushed its bns\")\n",
    "#                     ps_wei = ray.get(ps.pull_weights.remote())\n",
    "#                     model.load_state_dict(ps_wei)\n",
    "#                     sync_bns_flag = False\n",
    "#                 if args.cuda: model.cuda()\n",
    "            \n",
    "                    \n",
    "                \n",
    "                \n",
    "                \n",
    "            local_step += 1\n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                print('The {} worker, Train Epoch: {} [{}/{} ({:.1f}%)]\\tLoss: {:.6f}'.format(\n",
    "                worker_index, epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data))\n",
    "                \n",
    "                for name,param in model.named_parameters():\n",
    "                    wk_writer.add_histogram(name, param, local_step)\n",
    "\n",
    "                wk_writer.add_scalar(\"Loss/worker_train\",loss,local_step)\n",
    "                wk_writer.add_scalar(\"Accuracy/worker_train\",batch_correct.float()/len(data),local_step)\n",
    "                \n",
    "        print(\"The {} worker finished its {} epoch with loss: {} and accuracy: {}\".format(\n",
    "            worker_index,\n",
    "            epoch,\n",
    "            avg_loss / float(len(train_loader.dataset)),\n",
    "            train_correct.float() / float(len(train_loader.dataset))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Distributed SSP CIFAR-10 Restnet train with network slimming')\n",
    "parser.add_argument('--ray-master',type=str,default='127.0.0.1')\n",
    "parser.add_argument('--redis-port',type=str,default='6379')\n",
    "parser.add_argument('--batch-size',type=int,default=64)\n",
    "parser.add_argument('--test-batch-size', type=int, default=64)\n",
    "parser.add_argument('--epochs', type=int, default=160)\n",
    "parser.add_argument('--start-epoch', default=0, type=int)\n",
    "parser.add_argument('--lr', type=float, default=0.1)\n",
    "parser.add_argument('--momentum', type=float, default=0.9)\n",
    "parser.add_argument('--weight-decay', '--wd', default=1e-4, type=float)\n",
    "parser.add_argument('--resume', default=None, type=str) \n",
    "parser.add_argument('--no-cuda', action='store_true', default=False)\n",
    "parser.add_argument('--save', default='./logs', type=str)\n",
    "parser.add_argument('--depth', default=164, type=int)\n",
    "parser.add_argument('--tb-path', default='./logs', type=str)\n",
    "parser.add_argument('--log-interval', type=int, default=100)\n",
    "parser.add_argument('--num-workers',type=int,default=1)\n",
    "parser.add_argument('--stalness-limit',type=int,default=5)\n",
    "parser.add_argument('--debug',action='store_true',default=False)\n",
    "parser.add_argument('--sync-bns',type=int, default=194)\n",
    "\n",
    "args = parser.parse_args(args=['--num-workers=3'])\n",
    "# '--resume=/userhome/34/gyu/logs/checkpoint.pth.tar'\n",
    "# '--tb-path=logs_no_bns','--save=logs_no_bns'\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ray.is_initialized():\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '10.21.5.171',\n",
       " 'redis_address': '10.21.5.171:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2019-12-05_14-53-20_978335_17868/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2019-12-05_14-53-20_978335_17868/sockets/raylet',\n",
       " 'webui_url': 'http://10.21.5.171:8080/?token=6afbafabcdbb1b0f41cd13f86a44c962d1cc3f4a279b841d',\n",
       " 'session_dir': '/tmp/ray/session_2019-12-05_14-53-20_978335_17868'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(address=args.ray_master+':'+args.redis_port)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-05 15:45:15,086\tWARNING actor.py:678 -- Actor is garbage collected in the wrong driver. Actor id = ActorID(7d58f41501000000), class name = ParameterServer.\n"
     ]
    }
   ],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "\n",
    "test_loader = generate_test_loader(args.test_batch_size)\n",
    "train_loaders = [generate_train_loader(args.batch_size,kwargs) for _ in range(args.num_workers)]\n",
    "\n",
    "resume_from_ckpt = args.resume if (args.resume and os.path.isfile(args.resume)) else None\n",
    "\n",
    "ps = ParameterServer.remote(args,test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m worker # 0  is online\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 0 [0/50000 (0.0%)]\tLoss: 5.743666\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m worker # 1  is online\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 0 [0/50000 (0.0%)]\tLoss: 18.259109\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m worker # 2  is online\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 0 [0/50000 (0.0%)]\tLoss: 20.520304\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 0 [6400/50000 (12.8%)]\tLoss: 2.148239\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 0 [6400/50000 (12.8%)]\tLoss: 2.165159\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 0 [6400/50000 (12.8%)]\tLoss: 2.245616\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 0 [12800/50000 (25.6%)]\tLoss: 2.101958\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 0 [12800/50000 (25.6%)]\tLoss: 2.068329\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 0 [12800/50000 (25.6%)]\tLoss: 2.093445\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 0 [19200/50000 (38.4%)]\tLoss: 2.117046\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 0 [19200/50000 (38.4%)]\tLoss: 2.100170\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 0 [19200/50000 (38.4%)]\tLoss: 1.955357\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 0 [25600/50000 (51.2%)]\tLoss: 1.814547\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 0 [25600/50000 (51.2%)]\tLoss: 1.803103\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 0 [25600/50000 (51.2%)]\tLoss: 2.175115\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 0 [32000/50000 (63.9%)]\tLoss: 1.847991\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 0 [32000/50000 (63.9%)]\tLoss: 1.802682\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 0 [32000/50000 (63.9%)]\tLoss: 1.889183\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 0 [38400/50000 (76.7%)]\tLoss: 1.696224\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 0 [38400/50000 (76.7%)]\tLoss: 1.932145\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 0 [38400/50000 (76.7%)]\tLoss: 1.784084\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 0 [44800/50000 (89.5%)]\tLoss: 1.768714\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 0 [44800/50000 (89.5%)]\tLoss: 1.820931\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 0 [44800/50000 (89.5%)]\tLoss: 1.635356\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker finished its 0 epoch with loss: 0.04055042937397957 and accuracy: 0.2582800090312958\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker finished its 0 epoch with loss: 0.038807328790426254 and accuracy: 0.26263999938964844\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 1 [0/50000 (0.0%)]\tLoss: 1.840541\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 1 [0/50000 (0.0%)]\tLoss: 1.902948\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker finished its 0 epoch with loss: 0.03870898485183716 and accuracy: 0.2603200078010559\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 1 [0/50000 (0.0%)]\tLoss: 1.701508\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 1 [6400/50000 (12.8%)]\tLoss: 1.748492\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 1 [6400/50000 (12.8%)]\tLoss: 1.769206\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 1 [6400/50000 (12.8%)]\tLoss: 1.653175\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 1 [12800/50000 (25.6%)]\tLoss: 1.791122\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 1 [12800/50000 (25.6%)]\tLoss: 1.806868\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 1 [12800/50000 (25.6%)]\tLoss: 1.657620\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 1 [19200/50000 (38.4%)]\tLoss: 1.718262\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 1 [19200/50000 (38.4%)]\tLoss: 1.635272\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 1 [19200/50000 (38.4%)]\tLoss: 1.801568\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 1 [25600/50000 (51.2%)]\tLoss: 1.696943\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 1 [25600/50000 (51.2%)]\tLoss: 1.590618\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 1 [25600/50000 (51.2%)]\tLoss: 1.633676\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 1 [32000/50000 (63.9%)]\tLoss: 2.064906\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 1 [32000/50000 (63.9%)]\tLoss: 1.867677\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 1 [32000/50000 (63.9%)]\tLoss: 1.720188\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 1 [38400/50000 (76.7%)]\tLoss: 1.941138\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 1 [38400/50000 (76.7%)]\tLoss: 1.707489\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 1 [38400/50000 (76.7%)]\tLoss: 1.793197\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 1 [44800/50000 (89.5%)]\tLoss: 1.564627\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 1 [44800/50000 (89.5%)]\tLoss: 1.517132\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 1 [44800/50000 (89.5%)]\tLoss: 1.679058\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker finished its 1 epoch with loss: 0.0270216204226017 and accuracy: 0.37373998761177063\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker finished its 1 epoch with loss: 0.02700280211865902 and accuracy: 0.37303999066352844\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 2 [0/50000 (0.0%)]\tLoss: 1.665509\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 2 [0/50000 (0.0%)]\tLoss: 1.552217\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker finished its 1 epoch with loss: 0.026932919397950172 and accuracy: 0.3771800100803375\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 2 [0/50000 (0.0%)]\tLoss: 1.661003\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 2 [6400/50000 (12.8%)]\tLoss: 1.528515\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 2 [6400/50000 (12.8%)]\tLoss: 1.514008\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 2 [6400/50000 (12.8%)]\tLoss: 1.565204\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 2 [12800/50000 (25.6%)]\tLoss: 1.420116\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 2 [12800/50000 (25.6%)]\tLoss: 1.535660\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 2 [12800/50000 (25.6%)]\tLoss: 1.587701\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 2 [19200/50000 (38.4%)]\tLoss: 1.627031\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 2 [19200/50000 (38.4%)]\tLoss: 1.497958\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 2 [19200/50000 (38.4%)]\tLoss: 1.617121\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 2 [25600/50000 (51.2%)]\tLoss: 1.610195\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 2 [25600/50000 (51.2%)]\tLoss: 1.513067\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 2 [25600/50000 (51.2%)]\tLoss: 1.699756\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 2 [32000/50000 (63.9%)]\tLoss: 1.593593\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 2 [32000/50000 (63.9%)]\tLoss: 1.692484\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 2 [32000/50000 (63.9%)]\tLoss: 1.512703\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 2 [38400/50000 (76.7%)]\tLoss: 1.498909\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 2 [38400/50000 (76.7%)]\tLoss: 1.455833\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 2 [38400/50000 (76.7%)]\tLoss: 1.662734\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 2 [44800/50000 (89.5%)]\tLoss: 1.502500\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 2 [44800/50000 (89.5%)]\tLoss: 1.485642\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 2 [44800/50000 (89.5%)]\tLoss: 1.790176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker finished its 2 epoch with loss: 0.024714648723602295 and accuracy: 0.43050000071525574\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker finished its 2 epoch with loss: 0.024671431630849838 and accuracy: 0.4288400113582611\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 3 [0/50000 (0.0%)]\tLoss: 1.318744\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 3 [0/50000 (0.0%)]\tLoss: 1.446025\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker finished its 2 epoch with loss: 0.024657024070620537 and accuracy: 0.4309000074863434\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 3 [0/50000 (0.0%)]\tLoss: 1.500044\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 3 [6400/50000 (12.8%)]\tLoss: 1.548104\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 3 [6400/50000 (12.8%)]\tLoss: 1.458210\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 3 [6400/50000 (12.8%)]\tLoss: 1.387536\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 3 [12800/50000 (25.6%)]\tLoss: 1.627578\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 3 [12800/50000 (25.6%)]\tLoss: 1.769564\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 3 [12800/50000 (25.6%)]\tLoss: 1.496219\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 3 [19200/50000 (38.4%)]\tLoss: 1.291190\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 3 [19200/50000 (38.4%)]\tLoss: 1.342511\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 3 [19200/50000 (38.4%)]\tLoss: 1.459537\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 3 [25600/50000 (51.2%)]\tLoss: 1.250079\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 3 [25600/50000 (51.2%)]\tLoss: 1.217806\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 3 [25600/50000 (51.2%)]\tLoss: 1.406601\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 3 [32000/50000 (63.9%)]\tLoss: 1.274598\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 3 [32000/50000 (63.9%)]\tLoss: 1.485724\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 3 [32000/50000 (63.9%)]\tLoss: 1.635464\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 3 [38400/50000 (76.7%)]\tLoss: 1.593464\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 3 [38400/50000 (76.7%)]\tLoss: 1.504613\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 3 [38400/50000 (76.7%)]\tLoss: 1.461344\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 3 [44800/50000 (89.5%)]\tLoss: 1.622209\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 3 [44800/50000 (89.5%)]\tLoss: 1.884980\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 3 [44800/50000 (89.5%)]\tLoss: 1.410490\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker finished its 3 epoch with loss: 0.023182259872555733 and accuracy: 0.46522000432014465\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker finished its 3 epoch with loss: 0.023138610646128654 and accuracy: 0.4679200053215027\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 4 [0/50000 (0.0%)]\tLoss: 1.268921\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 4 [0/50000 (0.0%)]\tLoss: 1.268765\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker finished its 3 epoch with loss: 0.023221731185913086 and accuracy: 0.4657000005245209\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 4 [0/50000 (0.0%)]\tLoss: 1.299304\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 4 [6400/50000 (12.8%)]\tLoss: 1.417789\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 4 [6400/50000 (12.8%)]\tLoss: 1.560954\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 4 [6400/50000 (12.8%)]\tLoss: 1.610155\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 4 [12800/50000 (25.6%)]\tLoss: 1.208074\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 4 [12800/50000 (25.6%)]\tLoss: 1.661669\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 4 [12800/50000 (25.6%)]\tLoss: 1.259490\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 4 [19200/50000 (38.4%)]\tLoss: 1.344615\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 4 [19200/50000 (38.4%)]\tLoss: 1.375780\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 4 [19200/50000 (38.4%)]\tLoss: 1.312657\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 4 [25600/50000 (51.2%)]\tLoss: 1.403277\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 4 [25600/50000 (51.2%)]\tLoss: 1.219737\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 4 [25600/50000 (51.2%)]\tLoss: 1.358681\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 4 [32000/50000 (63.9%)]\tLoss: 1.375412\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 4 [32000/50000 (63.9%)]\tLoss: 1.483233\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 4 [32000/50000 (63.9%)]\tLoss: 1.125817\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 4 [38400/50000 (76.7%)]\tLoss: 1.238088\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 4 [38400/50000 (76.7%)]\tLoss: 1.483269\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 4 [38400/50000 (76.7%)]\tLoss: 1.298945\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 4 [44800/50000 (89.5%)]\tLoss: 1.306884\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 4 [44800/50000 (89.5%)]\tLoss: 1.428114\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 4 [44800/50000 (89.5%)]\tLoss: 1.389470\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker finished its 4 epoch with loss: 0.021674249321222305 and accuracy: 0.5038800239562988\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker finished its 4 epoch with loss: 0.021657021716237068 and accuracy: 0.5042799711227417\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 5 [0/50000 (0.0%)]\tLoss: 1.340627\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker finished its 4 epoch with loss: 0.021607201546430588 and accuracy: 0.5052199959754944\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 5 [0/50000 (0.0%)]\tLoss: 1.124106\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 5 [0/50000 (0.0%)]\tLoss: 1.158492\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 5 [6400/50000 (12.8%)]\tLoss: 1.191445\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 5 [6400/50000 (12.8%)]\tLoss: 1.001997\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 5 [6400/50000 (12.8%)]\tLoss: 1.240776\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 5 [12800/50000 (25.6%)]\tLoss: 1.405781\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 5 [12800/50000 (25.6%)]\tLoss: 1.344618\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 5 [12800/50000 (25.6%)]\tLoss: 1.240548\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 5 [19200/50000 (38.4%)]\tLoss: 1.301015\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 5 [19200/50000 (38.4%)]\tLoss: 1.328880\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 5 [19200/50000 (38.4%)]\tLoss: 1.479114\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 5 [25600/50000 (51.2%)]\tLoss: 1.265552\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 5 [25600/50000 (51.2%)]\tLoss: 1.291965\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 5 [25600/50000 (51.2%)]\tLoss: 1.365696\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 5 [32000/50000 (63.9%)]\tLoss: 1.194040\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 5 [32000/50000 (63.9%)]\tLoss: 1.442183\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 5 [32000/50000 (63.9%)]\tLoss: 0.976706\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 5 [38400/50000 (76.7%)]\tLoss: 1.185693\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 5 [38400/50000 (76.7%)]\tLoss: 1.154485\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 5 [38400/50000 (76.7%)]\tLoss: 1.414124\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 5 [44800/50000 (89.5%)]\tLoss: 1.242427\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 5 [44800/50000 (89.5%)]\tLoss: 1.130697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 5 [44800/50000 (89.5%)]\tLoss: 0.950957\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker finished its 5 epoch with loss: 0.019414596259593964 and accuracy: 0.5579800009727478\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker finished its 5 epoch with loss: 0.019409097731113434 and accuracy: 0.5584999918937683\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 6 [0/50000 (0.0%)]\tLoss: 1.234823\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 6 [0/50000 (0.0%)]\tLoss: 0.921237\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker finished its 5 epoch with loss: 0.019476091489195824 and accuracy: 0.5571799874305725\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 6 [0/50000 (0.0%)]\tLoss: 1.126369\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 6 [6400/50000 (12.8%)]\tLoss: 0.893920\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 6 [6400/50000 (12.8%)]\tLoss: 1.087798\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 6 [6400/50000 (12.8%)]\tLoss: 1.276693\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 6 [12800/50000 (25.6%)]\tLoss: 0.882283\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 6 [12800/50000 (25.6%)]\tLoss: 1.187314\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 6 [12800/50000 (25.6%)]\tLoss: 1.039159\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 6 [19200/50000 (38.4%)]\tLoss: 1.376073\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 6 [19200/50000 (38.4%)]\tLoss: 1.023967\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 6 [19200/50000 (38.4%)]\tLoss: 0.845164\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 6 [25600/50000 (51.2%)]\tLoss: 1.349601\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 6 [25600/50000 (51.2%)]\tLoss: 1.155571\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 6 [25600/50000 (51.2%)]\tLoss: 1.188271\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 6 [32000/50000 (63.9%)]\tLoss: 1.206667\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 6 [32000/50000 (63.9%)]\tLoss: 1.423246\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 6 [32000/50000 (63.9%)]\tLoss: 1.280046\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 6 [38400/50000 (76.7%)]\tLoss: 1.106961\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 6 [38400/50000 (76.7%)]\tLoss: 1.254285\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 6 [38400/50000 (76.7%)]\tLoss: 1.117020\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 6 [44800/50000 (89.5%)]\tLoss: 0.899454\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 6 [44800/50000 (89.5%)]\tLoss: 0.874675\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 6 [44800/50000 (89.5%)]\tLoss: 1.212128\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker finished its 6 epoch with loss: 0.01763073168694973 and accuracy: 0.6018400192260742\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker finished its 6 epoch with loss: 0.017630264163017273 and accuracy: 0.6019999980926514\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 7 [0/50000 (0.0%)]\tLoss: 0.888044\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker finished its 6 epoch with loss: 0.01774599216878414 and accuracy: 0.5967599749565125\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 7 [0/50000 (0.0%)]\tLoss: 0.933605\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 7 [0/50000 (0.0%)]\tLoss: 0.931955\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 7 [6400/50000 (12.8%)]\tLoss: 0.951296\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 7 [6400/50000 (12.8%)]\tLoss: 1.316838\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 7 [6400/50000 (12.8%)]\tLoss: 1.014053\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 7 [12800/50000 (25.6%)]\tLoss: 1.262806\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 7 [12800/50000 (25.6%)]\tLoss: 1.010208\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 7 [12800/50000 (25.6%)]\tLoss: 1.393402\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 7 [19200/50000 (38.4%)]\tLoss: 1.165929\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 7 [19200/50000 (38.4%)]\tLoss: 1.282195\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 7 [19200/50000 (38.4%)]\tLoss: 1.304824\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 7 [25600/50000 (51.2%)]\tLoss: 0.980870\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 7 [25600/50000 (51.2%)]\tLoss: 1.068144\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 7 [25600/50000 (51.2%)]\tLoss: 1.067827\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 7 [32000/50000 (63.9%)]\tLoss: 0.954410\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 7 [32000/50000 (63.9%)]\tLoss: 0.907605\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 7 [32000/50000 (63.9%)]\tLoss: 1.086040\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 7 [38400/50000 (76.7%)]\tLoss: 0.879503\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 7 [38400/50000 (76.7%)]\tLoss: 1.020151\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 7 [38400/50000 (76.7%)]\tLoss: 1.007854\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 7 [44800/50000 (89.5%)]\tLoss: 0.917936\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 7 [44800/50000 (89.5%)]\tLoss: 1.148192\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 7 [44800/50000 (89.5%)]\tLoss: 1.498031\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker finished its 7 epoch with loss: 0.016106506809592247 and accuracy: 0.6348199844360352\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker finished its 7 epoch with loss: 0.015984637662768364 and accuracy: 0.6381999850273132\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 8 [0/50000 (0.0%)]\tLoss: 0.776122\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 8 [0/50000 (0.0%)]\tLoss: 0.963785\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker finished its 7 epoch with loss: 0.01612047292292118 and accuracy: 0.6372799873352051\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 8 [0/50000 (0.0%)]\tLoss: 0.897596\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 8 [6400/50000 (12.8%)]\tLoss: 0.971663\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 8 [6400/50000 (12.8%)]\tLoss: 1.035780\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 8 [6400/50000 (12.8%)]\tLoss: 0.906723\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 8 [12800/50000 (25.6%)]\tLoss: 0.810445\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 8 [12800/50000 (25.6%)]\tLoss: 0.861790\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 8 [12800/50000 (25.6%)]\tLoss: 0.919097\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 8 [19200/50000 (38.4%)]\tLoss: 0.799808\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 8 [19200/50000 (38.4%)]\tLoss: 0.701623\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 8 [19200/50000 (38.4%)]\tLoss: 0.806504\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 8 [25600/50000 (51.2%)]\tLoss: 1.064481\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 8 [25600/50000 (51.2%)]\tLoss: 1.114281\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 8 [25600/50000 (51.2%)]\tLoss: 0.786746\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 8 [32000/50000 (63.9%)]\tLoss: 0.694278\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 8 [32000/50000 (63.9%)]\tLoss: 0.941739\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 8 [32000/50000 (63.9%)]\tLoss: 0.976799\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 8 [38400/50000 (76.7%)]\tLoss: 0.616232\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 8 [38400/50000 (76.7%)]\tLoss: 0.885851\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 8 [38400/50000 (76.7%)]\tLoss: 0.943904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 8 [44800/50000 (89.5%)]\tLoss: 0.934295\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 8 [44800/50000 (89.5%)]\tLoss: 0.961433\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 8 [44800/50000 (89.5%)]\tLoss: 0.948544\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker finished its 8 epoch with loss: 0.013450773432850838 and accuracy: 0.698199987411499\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker finished its 8 epoch with loss: 0.013541066087782383 and accuracy: 0.6970999836921692\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 9 [0/50000 (0.0%)]\tLoss: 0.931571\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 9 [0/50000 (0.0%)]\tLoss: 0.728234\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker finished its 8 epoch with loss: 0.013458478264510632 and accuracy: 0.699940025806427\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 9 [0/50000 (0.0%)]\tLoss: 0.756329\n"
     ]
    }
   ],
   "source": [
    "worker_tasks = [worker_task.remote(args,ps,idx,train_loaders[idx]) for idx in range(args.num_workers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 21 [12800/50000 (25.6%)]\tLoss: 0.308957\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 21 [12800/50000 (25.6%)]\tLoss: 0.243833\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 21 [12800/50000 (25.6%)]\tLoss: 0.210239\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 21 [19200/50000 (38.4%)]\tLoss: 0.297020\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 21 [19200/50000 (38.4%)]\tLoss: 0.430077\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 21 [19200/50000 (38.4%)]\tLoss: 0.224828\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 21 [25600/50000 (51.2%)]\tLoss: 0.150578\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 21 [25600/50000 (51.2%)]\tLoss: 0.294282\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 21 [25600/50000 (51.2%)]\tLoss: 0.330981\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 21 [32000/50000 (63.9%)]\tLoss: 0.177099\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 21 [32000/50000 (63.9%)]\tLoss: 0.241877\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 21 [32000/50000 (63.9%)]\tLoss: 0.345868\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 21 [38400/50000 (76.7%)]\tLoss: 0.252502\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 21 [38400/50000 (76.7%)]\tLoss: 0.400659\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 21 [38400/50000 (76.7%)]\tLoss: 0.398720\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 21 [44800/50000 (89.5%)]\tLoss: 0.218433\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 21 [44800/50000 (89.5%)]\tLoss: 0.103535\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 21 [44800/50000 (89.5%)]\tLoss: 0.201365\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker finished its 21 epoch with loss: 0.00375644164159894 and accuracy: 0.9160599708557129\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker finished its 21 epoch with loss: 0.003850584849715233 and accuracy: 0.9132000207901001\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 22 [0/50000 (0.0%)]\tLoss: 0.109714\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 22 [0/50000 (0.0%)]\tLoss: 0.343161\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker finished its 21 epoch with loss: 0.00378871476277709 and accuracy: 0.915880024433136\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 22 [0/50000 (0.0%)]\tLoss: 0.348527\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 22 [6400/50000 (12.8%)]\tLoss: 0.088594\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 22 [6400/50000 (12.8%)]\tLoss: 0.162088\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 22 [6400/50000 (12.8%)]\tLoss: 0.351341\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 22 [12800/50000 (25.6%)]\tLoss: 0.139441\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 22 [12800/50000 (25.6%)]\tLoss: 0.231936\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 22 [12800/50000 (25.6%)]\tLoss: 0.333796\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 22 [19200/50000 (38.4%)]\tLoss: 0.365304\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 22 [19200/50000 (38.4%)]\tLoss: 0.312295\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 22 [19200/50000 (38.4%)]\tLoss: 0.228725\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 22 [25600/50000 (51.2%)]\tLoss: 0.197911\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 22 [25600/50000 (51.2%)]\tLoss: 0.166331\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 22 [25600/50000 (51.2%)]\tLoss: 0.238296\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 22 [32000/50000 (63.9%)]\tLoss: 0.213404\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 22 [32000/50000 (63.9%)]\tLoss: 0.357051\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 22 [32000/50000 (63.9%)]\tLoss: 0.091496\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 22 [38400/50000 (76.7%)]\tLoss: 0.291733\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 22 [38400/50000 (76.7%)]\tLoss: 0.210852\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 22 [38400/50000 (76.7%)]\tLoss: 0.333561\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 22 [44800/50000 (89.5%)]\tLoss: 0.147938\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 22 [44800/50000 (89.5%)]\tLoss: 0.207760\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 22 [44800/50000 (89.5%)]\tLoss: 0.131049\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker finished its 22 epoch with loss: 0.0036460247356444597 and accuracy: 0.9182599782943726\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker finished its 22 epoch with loss: 0.0036702940706163645 and accuracy: 0.9179999828338623\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 23 [0/50000 (0.0%)]\tLoss: 0.283992\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 23 [0/50000 (0.0%)]\tLoss: 0.297908\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker finished its 22 epoch with loss: 0.0036380342207849026 and accuracy: 0.9184600114822388\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 23 [0/50000 (0.0%)]\tLoss: 0.151074\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 23 [6400/50000 (12.8%)]\tLoss: 0.235096\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 23 [6400/50000 (12.8%)]\tLoss: 0.197298\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 23 [6400/50000 (12.8%)]\tLoss: 0.099795\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 23 [12800/50000 (25.6%)]\tLoss: 0.212384\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 23 [12800/50000 (25.6%)]\tLoss: 0.199371\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 23 [12800/50000 (25.6%)]\tLoss: 0.084584\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 23 [19200/50000 (38.4%)]\tLoss: 0.141941\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 23 [19200/50000 (38.4%)]\tLoss: 0.194704\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 23 [19200/50000 (38.4%)]\tLoss: 0.129487\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 23 [25600/50000 (51.2%)]\tLoss: 0.215524\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 23 [25600/50000 (51.2%)]\tLoss: 0.124835\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 23 [25600/50000 (51.2%)]\tLoss: 0.276784\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 23 [32000/50000 (63.9%)]\tLoss: 0.359889\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 23 [32000/50000 (63.9%)]\tLoss: 0.204259\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 23 [32000/50000 (63.9%)]\tLoss: 0.063256\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 23 [38400/50000 (76.7%)]\tLoss: 0.211561\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 23 [38400/50000 (76.7%)]\tLoss: 0.220702\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 23 [38400/50000 (76.7%)]\tLoss: 0.110088\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 23 [44800/50000 (89.5%)]\tLoss: 0.348626\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 23 [44800/50000 (89.5%)]\tLoss: 0.157653\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 23 [44800/50000 (89.5%)]\tLoss: 0.185061\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker finished its 23 epoch with loss: 0.0034960494376719 and accuracy: 0.9209799766540527\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker finished its 23 epoch with loss: 0.0034326568711549044 and accuracy: 0.9221199750900269\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 24 [0/50000 (0.0%)]\tLoss: 0.146050\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 24 [0/50000 (0.0%)]\tLoss: 0.208621\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker finished its 23 epoch with loss: 0.0035200531128793955 and accuracy: 0.9210799932479858\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 24 [0/50000 (0.0%)]\tLoss: 0.164688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 24 [6400/50000 (12.8%)]\tLoss: 0.139090\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 24 [6400/50000 (12.8%)]\tLoss: 0.209795\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 24 [6400/50000 (12.8%)]\tLoss: 0.392703\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 24 [12800/50000 (25.6%)]\tLoss: 0.265441\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 24 [12800/50000 (25.6%)]\tLoss: 0.134546\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 24 [12800/50000 (25.6%)]\tLoss: 0.109226\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 24 [19200/50000 (38.4%)]\tLoss: 0.126671\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 24 [19200/50000 (38.4%)]\tLoss: 0.258924\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 24 [19200/50000 (38.4%)]\tLoss: 0.285979\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 24 [25600/50000 (51.2%)]\tLoss: 0.146260\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 24 [25600/50000 (51.2%)]\tLoss: 0.219174\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 24 [25600/50000 (51.2%)]\tLoss: 0.302929\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 24 [32000/50000 (63.9%)]\tLoss: 0.139557\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 24 [32000/50000 (63.9%)]\tLoss: 0.271556\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 24 [32000/50000 (63.9%)]\tLoss: 0.133470\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 24 [38400/50000 (76.7%)]\tLoss: 0.135468\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 24 [38400/50000 (76.7%)]\tLoss: 0.285728\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 24 [38400/50000 (76.7%)]\tLoss: 0.164497\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 24 [44800/50000 (89.5%)]\tLoss: 0.241722\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 24 [44800/50000 (89.5%)]\tLoss: 0.286482\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 24 [44800/50000 (89.5%)]\tLoss: 0.124691\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker finished its 24 epoch with loss: 0.0033512385562062263 and accuracy: 0.9241600036621094\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker finished its 24 epoch with loss: 0.003363002324476838 and accuracy: 0.9261000156402588\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 25 [0/50000 (0.0%)]\tLoss: 0.349185\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 25 [0/50000 (0.0%)]\tLoss: 0.188835\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker finished its 24 epoch with loss: 0.0033660982735455036 and accuracy: 0.9248200058937073\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 25 [0/50000 (0.0%)]\tLoss: 0.117266\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 25 [6400/50000 (12.8%)]\tLoss: 0.092215\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 25 [6400/50000 (12.8%)]\tLoss: 0.257466\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 25 [6400/50000 (12.8%)]\tLoss: 0.242324\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 25 [12800/50000 (25.6%)]\tLoss: 0.350491\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 25 [12800/50000 (25.6%)]\tLoss: 0.173543\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 25 [12800/50000 (25.6%)]\tLoss: 0.120320\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 25 [19200/50000 (38.4%)]\tLoss: 0.281215\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 25 [19200/50000 (38.4%)]\tLoss: 0.322370\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 25 [19200/50000 (38.4%)]\tLoss: 0.236641\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 25 [25600/50000 (51.2%)]\tLoss: 0.085454\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 25 [25600/50000 (51.2%)]\tLoss: 0.314055\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 25 [25600/50000 (51.2%)]\tLoss: 0.187943\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 25 [32000/50000 (63.9%)]\tLoss: 0.245770\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 25 [32000/50000 (63.9%)]\tLoss: 0.217867\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 25 [32000/50000 (63.9%)]\tLoss: 0.119036\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 25 [38400/50000 (76.7%)]\tLoss: 0.276229\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 25 [38400/50000 (76.7%)]\tLoss: 0.249615\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 25 [38400/50000 (76.7%)]\tLoss: 0.160057\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 25 [44800/50000 (89.5%)]\tLoss: 0.214405\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 25 [44800/50000 (89.5%)]\tLoss: 0.288122\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 25 [44800/50000 (89.5%)]\tLoss: 0.114260\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker finished its 25 epoch with loss: 0.0032768428791314363 and accuracy: 0.9257799983024597\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker finished its 25 epoch with loss: 0.0032924541737884283 and accuracy: 0.9259799718856812\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 26 [0/50000 (0.0%)]\tLoss: 0.189706\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker finished its 25 epoch with loss: 0.0032450456637889147 and accuracy: 0.9266200065612793\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 26 [0/50000 (0.0%)]\tLoss: 0.221507\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 26 [0/50000 (0.0%)]\tLoss: 0.250997\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 26 [6400/50000 (12.8%)]\tLoss: 0.201291\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 26 [6400/50000 (12.8%)]\tLoss: 0.189659\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 26 [6400/50000 (12.8%)]\tLoss: 0.207025\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 26 [12800/50000 (25.6%)]\tLoss: 0.148201\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 26 [12800/50000 (25.6%)]\tLoss: 0.209990\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 26 [12800/50000 (25.6%)]\tLoss: 0.116997\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 26 [19200/50000 (38.4%)]\tLoss: 0.387546\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 26 [19200/50000 (38.4%)]\tLoss: 0.122813\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 26 [19200/50000 (38.4%)]\tLoss: 0.284608\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 26 [25600/50000 (51.2%)]\tLoss: 0.174188\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 26 [25600/50000 (51.2%)]\tLoss: 0.223036\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 26 [25600/50000 (51.2%)]\tLoss: 0.336972\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (6): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (7): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (8): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (9): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (10): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (11): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (12): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (13): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (14): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (15): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (16): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (17): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (18): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (19): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (20): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (21): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (22): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (23): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (24): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (25): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (26): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (6): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (7): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (8): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (9): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (10): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (11): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (12): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (13): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (14): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (15): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (16): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (17): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (18): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (19): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (20): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (21): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (22): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (23): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (24): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (25): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (26): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (6): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (7): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (8): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (9): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (10): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (11): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (12): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (13): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (14): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (15): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (16): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (17): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (18): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (19): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (20): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (21): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (22): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (23): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (24): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (25): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (26): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
       "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_test_loader = generate_test_loader(64)\n",
    "local_train_loader = generate_train_loader(64,{'num_workers': 1, 'pin_memory': True})\n",
    "\n",
    "test_writer = SummaryWriter()\n",
    "\n",
    "local_test_model = models.__dict__[\"resnet\"](dataset=\"cifar10\",depth=args.depth)\n",
    "checkpoint = torch.load('/userhome/34/gyu/logs/checkpoint.pth.tar')\n",
    "local_test_model.load_state_dict(checkpoint['state_dict'])\n",
    "local_test_model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_dataloader):\n",
    "    local_test_model = model\n",
    "    local_test_loader = test_dataloader\n",
    "    # local_test_model.train()\n",
    "    local_test_model.eval()\n",
    "    # test dataset loader \n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    batch_count = 0.\n",
    "    for data, target in local_test_loader:\n",
    "        data,target = data.cuda(),target.cuda()\n",
    "        batch_count += 1\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = local_test_model(data)\n",
    "        test_loss += F.cross_entropy(output, target, size_average=False).data # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        batch_correct = pred.eq(target.data.view_as(pred)).sum()\n",
    "        correct += batch_correct\n",
    "#         if batch_count % 100  == 0:\n",
    "#             print(\"        with model.eval(), batch num: \",batch_count, \" with correct: \",int(batch_correct.data), \" / \",len(data))\n",
    "\n",
    "    test_loss /= len(local_test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.6f})\\n'.format(\n",
    "        test_loss, correct, len(local_test_loader.dataset),\n",
    "        correct / float(len(local_test_loader.dataset))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_test_model.train()\n",
    "# test_model(local_test_model,local_test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 5.4273, Accuracy: 1396/10000 (0.000000)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ipykernel_launcher.py:23: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 33 [19200/50000 (38.4%)]\tLoss: 0.151407\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 33 [19200/50000 (38.4%)]\tLoss: 0.266747\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 33 [19200/50000 (38.4%)]\tLoss: 0.223348\n",
      "10.0\n",
      "\n",
      "Test set: Average loss: 2.2098, Accuracy: 1824/10000 (0.000000)\n",
      "\n",
      "20.0\n",
      "\n",
      "Test set: Average loss: 0.9871, Accuracy: 7004/10000 (0.000000)\n",
      "\n",
      "30.0\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 33 [25600/50000 (51.2%)]\tLoss: 0.139510\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 33 [25600/50000 (51.2%)]\tLoss: 0.173494\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 33 [25600/50000 (51.2%)]\tLoss: 0.181789\n",
      "\n",
      "Test set: Average loss: 0.4489, Accuracy: 8526/10000 (0.000000)\n",
      "\n",
      "40.0\n",
      "\n",
      "Test set: Average loss: 0.4247, Accuracy: 8711/10000 (0.000000)\n",
      "\n",
      "50.0\n",
      "\n",
      "Test set: Average loss: 0.4270, Accuracy: 8738/10000 (0.000000)\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 33 [32000/50000 (63.9%)]\tLoss: 0.092215\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 33 [32000/50000 (63.9%)]\tLoss: 0.343507\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 33 [32000/50000 (63.9%)]\tLoss: 0.165036\n",
      "60.0\n",
      "\n",
      "Test set: Average loss: 0.4355, Accuracy: 8719/10000 (0.000000)\n",
      "\n",
      "70.0\n",
      "\n",
      "Test set: Average loss: 0.4320, Accuracy: 8730/10000 (0.000000)\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 33 [38400/50000 (76.7%)]\tLoss: 0.174756\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 33 [38400/50000 (76.7%)]\tLoss: 0.115467\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 33 [38400/50000 (76.7%)]\tLoss: 0.107634\n",
      "80.0\n",
      "\n",
      "Test set: Average loss: 0.4339, Accuracy: 8728/10000 (0.000000)\n",
      "\n",
      "90.0\n",
      "\n",
      "Test set: Average loss: 0.4339, Accuracy: 8722/10000 (0.000000)\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=13523, ip=10.21.5.172)\u001b[0m The 2 worker, Train Epoch: 33 [44800/50000 (89.5%)]\tLoss: 0.204827\n",
      "\u001b[2m\u001b[36m(pid=29165, ip=10.21.5.173)\u001b[0m The 0 worker, Train Epoch: 33 [44800/50000 (89.5%)]\tLoss: 0.162663\n",
      "100.0\n",
      "\u001b[2m\u001b[36m(pid=17901)\u001b[0m The 1 worker, Train Epoch: 33 [44800/50000 (89.5%)]\tLoss: 0.292835\n",
      "\n",
      "Test set: Average loss: 0.4405, Accuracy: 8713/10000 (0.000000)\n",
      "\n",
      "\n",
      "Train set: Average loss: 0.0241, Accuracy: 5975/50000 (0.000000)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train dataset loader, but set model.eval(), acc=11% for one epoch \n",
    "# local_test_model.eval()\n",
    "local_test_model.train()\n",
    "# train dataset loader\n",
    "\n",
    "test_loss = 0.\n",
    "correct = 0.\n",
    "train_batch_count = 0.\n",
    "num_batch = 0\n",
    "for data, target in local_train_loader:\n",
    "    if train_batch_count % 10 == 0:\n",
    "        print(train_batch_count)\n",
    "        local_test_model.eval()\n",
    "        test_model(local_test_model,local_test_loader)\n",
    "        \n",
    "    if train_batch_count==100:\n",
    "        break\n",
    "        \n",
    "    num_batch += 1   \n",
    "    local_test_model.train()\n",
    "    data,target = data.cuda(),target.cuda()\n",
    "    train_batch_count += 1\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    output = local_test_model(data)\n",
    "    test_loss += F.cross_entropy(output, target, size_average=False).data # sum up batch loss\n",
    "    pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "    batch_correct = pred.eq(target.data.view_as(pred)).sum()\n",
    "    correct += batch_correct\n",
    "    \n",
    "    for name,param in local_test_model.named_parameters():\n",
    "        test_writer.add_histogram(name, param, num_batch)\n",
    "\n",
    "    \n",
    "#     if train_batch_count % 100  == 0:\n",
    "#         print(\"With model.train(), batch num: \",train_batch_count, \" , with correct: \",int(batch_correct.data), \" / \", len(data))\n",
    "\n",
    "test_loss /= len(local_train_loader.dataset)\n",
    "print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.6f})\\n'.format(\n",
    "    test_loss, correct, len(local_train_loader.dataset),\n",
    "    correct / float(len(local_train_loader.dataset))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_bns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_bns2,local_nonbns2 = _get_params(local_test_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_bns2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = ray.get(ps.pull_weights.remote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_bn={}\n",
    "for ele in state_dict:\n",
    "    if 'bn' not in ele:\n",
    "        non_bn[ele]=state_dict[ele]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "init_wei = ray.get(ps.pull_weights.remote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_wei[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
