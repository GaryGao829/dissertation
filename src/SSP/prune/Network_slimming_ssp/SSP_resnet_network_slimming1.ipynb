{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import ray\n",
    "import models\n",
    "import random,time\n",
    "from time import sleep\n",
    "import copy \n",
    "import datetime\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from filelock import FileLock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_loader(batch_size,kwargs):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10('./data.cifar10', train=True, download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.Pad(4),\n",
    "            transforms.RandomCrop(32),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    return train_loader\n",
    "\n",
    "def generate_test_loader(test_batch_size):\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10('./data.cifar10', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        ])),batch_size=test_batch_size, shuffle=True)\n",
    "    return test_loader\n",
    "\n",
    "def _get_params(model):\n",
    "    bns = {}\n",
    "    non_bns = {}\n",
    "    param_count = 0.\n",
    "    bn_param_count = 0.\n",
    "    for name,param in model.named_parameters():\n",
    "        param_count += len(param)\n",
    "        if 'bn' in name:\n",
    "            bns[name] = param\n",
    "            bn_param_count += len(param)\n",
    "        else:\n",
    "            non_bns[name] = param\n",
    "    print(\"bn params occupies: \",bn_param_count/param_count)\n",
    "    return bns,non_bns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class ParameterServer():\n",
    "    def __init__(self,args,test_loader):\n",
    "        self.model = models.__dict__[\"resnet\"](dataset=\"cifar10\",depth=args.depth)\n",
    "        self.stalness_table = [0] * args.num_workers\n",
    "        self.stalness_limit = args.stalness_limit \n",
    "        self.global_step = 0\n",
    "        self.lr = args.lr\n",
    "        self.args = args\n",
    "        self.eva_model = models.__dict__[\"resnet\"](dataset=\"cifar10\",depth=args.depth)\n",
    "        self.optimizer = optim.SGD(self.model.parameters(),\n",
    "                          lr=args.lr,\n",
    "                          momentum=args.momentum,\n",
    "                          weight_decay=args.weight_decay)\n",
    "        self.test_loader = test_loader\n",
    "        self.model.cpu()\n",
    "        self.eva_model.cpu()\n",
    "        self.ps_writer = SummaryWriter(os.path.join(os.getcwd(),(args.tb_path+'/ps')))\n",
    "        self.save_path = args.save\n",
    "        self.num_workers = (int)(args.num_workers)\n",
    "        \n",
    "        # get point to all non_bns parameters\n",
    "        self.non_bns = [param.data for name,param in self.model.named_parameters() if 'bn' not in name]\n",
    "        self.bns = [param.data for name,param in self.model.named_parameters() if 'bn' in name]\n",
    "        self.bns_sync = [None] * args.num_workers\n",
    "        self.cfg = None\n",
    "        self.finished = [0] * args.num_workers\n",
    "        \n",
    "        if args.refine:\n",
    "            if os.path.isfile(args.refine):\n",
    "                print('found pruned ckpt')\n",
    "                checkpoint = torch.load(args.refine)\n",
    "                self.cfg = checkpoint['cfg']\n",
    "                self.model = models.__dict__['resnet'](dataset='cifar10', depth=args.depth, cfg=checkpoint['cfg'])\n",
    "                \n",
    "                if args.resume:\n",
    "                    if os.path.isfile(args.resume):\n",
    "                        print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "                        checkpoint = torch.load(args.resume)\n",
    "                        self.global_step = checkpoint['global_step']\n",
    "                        if 'optimizer' in checkpoint:\n",
    "                            self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "                        self.stalness_table = [self.global_step/args.num_workers] * args.num_workers\n",
    "                    else:\n",
    "                        print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "                        \n",
    "                self.model.load_state_dict(checkpoint['state_dict'])\n",
    "                self.optimizer = optim.SGD(self.model.parameters(),\n",
    "                          lr=args.lr,\n",
    "                          momentum=args.momentum,\n",
    "                          weight_decay=args.weight_decay)\n",
    "\n",
    "        elif args.resume:\n",
    "            if os.path.isfile(args.resume):\n",
    "                print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "                checkpoint = torch.load(args.resume)\n",
    "                self.global_step = checkpoint['global_step']\n",
    "                self.model.load_state_dict(checkpoint['state_dict'])\n",
    "                self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "                self.stalness_table = [self.global_step/args.num_workers] * args.num_workers\n",
    "                print(\"=> loaded checkpoint '{}' (global step: {})\".format(args.resume, checkpoint['global_step']))                \n",
    "                if 'epoch' in checkpoint: print(\"epoch: {}\".format(checkpoint['epoch']))\n",
    "            else:\n",
    "                print(\"=> no checkpoint found at '{}'\".format(args.resume))                    \n",
    "                \n",
    "#     def apply_gradients(self, iter_diff, wk_idx, epoch):\n",
    "#         if args.debug: print(\"applying gradients from the \",wk_idx, \" worker\")\n",
    "        \n",
    "#         # updata all params\n",
    "#         for idx, p in enumerate(self.model.parameters()):\n",
    "#             p.data -= iter_diff[idx]\n",
    "\n",
    "# #         if wk_idx == 0:\n",
    "# #             # updata all params\n",
    "# #             for idx, p in enumerate(self.model.parameters()):\n",
    "# #                 p.data -= iter_diff[idx]\n",
    "# #         else:\n",
    "# #             # only update non_bns params\n",
    "# #             for idx, tensor in enumerate(self.non_bns):\n",
    "# #                 tensor -= iter_diff[idx]\n",
    "        \n",
    "# #         # only update non_bns params\n",
    "# #         for idx, tensor in enumerate(self.non_bns):\n",
    "# #             tensor -= iter_diff[idx]\n",
    "        \n",
    "#         self.stalness_table[wk_idx] += 1\n",
    "#         self.global_step += 1\n",
    "#         if args.debug: print(\"finished applying gradients from the \",wk_idx, \" worker\")\n",
    "#         if self.global_step % 1000 == 0:\n",
    "# #             print(\"global_step: \",self.global_step,\" and prepare evaluate\")\n",
    "# #             self.evaluate()\n",
    "#             self.save_ckpt({\n",
    "#                 'epoch':epoch,\n",
    "#                 'global_step':self.global_step,\n",
    "#                 'state_dict':self.model.state_dict(),\n",
    "#                 'optimizer':self.optimizer.state_dict()\n",
    "#             },filepath=os.path.join(os.getcwd(),self.save_path))\n",
    "    \n",
    "    def apply_gradients_with_running_bn(self,iter_diff,wk_idx,epoch):\n",
    "        state = self.model.state_dict()\n",
    "        for idx, p in enumerate(state):\n",
    "            state[p] -= iter_diff[idx]\n",
    "            \n",
    "        self.stalness_table[wk_idx] += 1\n",
    "        self.global_step += 1\n",
    "        if args.debug: print(\"finished applying gradients from the \",wk_idx, \" worker\")\n",
    "        if self.global_step % 782 == 0:\n",
    "            self.save_ckpt({\n",
    "                'epoch':epoch,\n",
    "                'global_step':self.global_step,\n",
    "                'state_dict':self.model.state_dict(),\n",
    "                'optimizer':self.optimizer.state_dict(),\n",
    "                'cfg':self.cfg\n",
    "            },filepath=os.path.join(os.getcwd(),self.save_path))\n",
    "            \n",
    "        if self.global_step % (782 * self.num_workers) == 0:\n",
    "            print(\"global_step: \",self.global_step,\" and prepare evaluate\")\n",
    "            self.evaluate()\n",
    "\n",
    "        # when all worker finished\n",
    "        if all(self.finished):\n",
    "            self.save_ckpt({\n",
    "                'epoch':epoch,\n",
    "                'global_step':self.global_step,\n",
    "                'state_dict':self.model.state_dict(),\n",
    "                'optimizer':self.optimizer.state_dict(),\n",
    "                'cfg':self.cfg\n",
    "            },filepath=os.path.join(os.getcwd(),self.save_path))\n",
    "            \n",
    "            print(\"All worker finished its job, and have saved the ckpt\")\n",
    "#     def apply_gradients_non_bns(self,iter_diff,wk_idx,epoch):\n",
    "#         for i in range(len(self.non_bns)):\n",
    "#             self.non_bns[i] -= iter_diff[i]\n",
    "# #         print(wk_idx,\"finished updating non bns on ps\")\n",
    "\n",
    "        \n",
    "#     def apply_gradients_bns(self,iter_diff,wk_idx,epoch):\n",
    "#         for idx, tensor in enumerate(self.bns):\n",
    "# #             if idx == len(iter_diff) / 2:\n",
    "# #                 print(wk_idx,\"is in the middle of updating bns on ps\")\n",
    "#             tensor -= iter_diff[idx]\n",
    "# #         print(wk_idx,\"finished updating non bns on ps\")\n",
    "# #         self.stalness_table[wk_idx] += 1\n",
    "# #         self.global_step += 1\n",
    "\n",
    "#     def apply_gradients_partical_bns(self, iter_diff, wk_idx, epoch):\n",
    "#         if wk_idx == 0 :\n",
    "#             for i in range(0,(int)(len(self.bns)/self.num_workers)):\n",
    "#                 self.bns[i] -= iter_diff[i]\n",
    "#         elif wk_idx == self.num_workers:\n",
    "#             for i in range((int)(len(self.bns) * wk_idx / self.num_workers + 1) , len(self.bns)):\n",
    "#                 self.bns[i] -= iter_diff[i]\n",
    "#         else:\n",
    "#             for i in range((int)(len(self.bns) * wk_idx / self.num_workers + 1) , (int)(len(self.bns) * (wk_idx + 1) / self.num_workers)):\n",
    "#                 self.bns[i] -= iter_diff[i]\n",
    "#         self.stalness_table[wk_idx] += 1\n",
    "#         self.global_step += 1\n",
    "#         if self.global_step % 1000 == 0:\n",
    "#             self.save_ckpt({\n",
    "#                 'epoch':epoch,\n",
    "#                 'global_step':self.global_step,\n",
    "#                 'state_dict':self.model.state_dict(),\n",
    "#                 'optimizer':self.optimizer.state_dict()\n",
    "#             },filepath=os.path.join(os.getcwd(),self.save_path))\n",
    "    def pull_cfg(self):\n",
    "        return self.cfg\n",
    "        \n",
    "    def pull_weights(self):\n",
    "        return self.model.state_dict()\n",
    "    \n",
    "    def pull_non_bn_weights(self):\n",
    "        self.non_bns = [param.data for name,param in self.model.named_parameters() if 'bn' not in name]\n",
    "        return copy.deepcopy(self.non_bns)\n",
    "    \n",
    "    def pull_bn_weights(self):\n",
    "        self.bns = [param.data for name,param in self.model.named_parameters() if 'bn' in name]\n",
    "        return copy.deepcopy(self.bns)\n",
    "    \n",
    "    def get_optim(self):\n",
    "        return self.optimizer\n",
    "    \n",
    "    def pull_optimizer_state(self):\n",
    "        return self.optimizer.state_dict()\n",
    "\n",
    "    def check_stalness(self,wk_idx):\n",
    "        min_iter = min(self.stalness_table)\n",
    "        return self.stalness_table[wk_idx] - min_iter < self.stalness_limit\n",
    "        \n",
    "    def get_stalness(self):\n",
    "        return min(self.stalness_table)\n",
    "    \n",
    "    def get_stalness_table(self):\n",
    "        return self.stalness_table\n",
    "    \n",
    "    def get_global_step(self):\n",
    "        return self.global_step\n",
    "    \n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def save_ckpt(self,state,filepath):\n",
    "        torch.save(state,os.path.join(filepath,'checkpoint.pth.tar'))\n",
    "    \n",
    "    def get_bns_ready(self):\n",
    "        return any(self.bns_sync) == False\n",
    "\n",
    "    def set_finished(self,worker_index):\n",
    "        print('worker:',worker_index,'has finished its job')\n",
    "        self.finished[worker_index] = 1\n",
    "        \n",
    "    def aggregate_bns(self,wk_bns,worker_index):\n",
    "        self.bns_sync[worker_index] = wk_bns\n",
    "        if all(self.bns_sync):\n",
    "            for i in range(len(self.bns)):\n",
    "                tmp = copy.deepcopy(self.bns_sync[0][i])\n",
    "                for j in range(1, self.num_workers):\n",
    "                    tmp += self.bns_sync[j][i]\n",
    "                self.bns[i] = tmp / self.num_workers    \n",
    "            self.bns_sync = [None] * self.num_workers\n",
    "        return self.bns_sync\n",
    "    \n",
    "    def evaluate(self):\n",
    "        print(\"going to evaluate\")\n",
    "        test_loss = 0.\n",
    "        correct = 0.\n",
    "        print(\"pulled weights\")\n",
    "        self.eva_model= copy.deepcopy(self.model)\n",
    "        print(\"loaded weights\")\n",
    "        print(\"length of the test_loader dataset is : \",len(self.test_loader.dataset))\n",
    "        self.eva_model.eval()\n",
    "        count = 0\n",
    "        for data,target in self.test_loader:\n",
    "            count += 1\n",
    "            if count % 20 == 0: print(\"in eval, the batch is: \",count)\n",
    "            data, target = Variable(data,volatile=True),Variable(target)\n",
    "            output = self.eva_model(data)\n",
    "            batch_loss = F.cross_entropy(output, target, size_average=False).data\n",
    "            test_loss += batch_loss\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        len_testset = len(self.test_loader.dataset)\n",
    "        test_loss /= len_testset \n",
    "        accuracy = correct.float() / len_testset\n",
    "        # log \n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.4f})\\n'.format(\n",
    "        test_loss, correct, len_testset,accuracy))\n",
    "\n",
    "        self.ps_writer.add_scalar('Accuracy/eval', accuracy, self.global_step)\n",
    "        self.ps_writer.add_scalar('Loss/eval',test_loss , self.global_step)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_gpus=1)\n",
    "def worker_task(args,ps,worker_index, train_loader):\n",
    "    # Initialize the model.\n",
    "#     if args.debug: print(worker_index, \" worker is going to sleep \",worker_index*5000)\n",
    "#     time.sleep(worker_index * 5000)\n",
    "    \n",
    "    model = models.__dict__[\"resnet\"](dataset=\"cifar10\",depth=args.depth)\n",
    "    local_step = 0\n",
    "    \n",
    "    wk_non_bns = [param.data for name,param in model.named_parameters() if 'bn' not in name]\n",
    "    wk_bns = [param.data for name,param in model.named_parameters() if 'bn' in name]\n",
    "    \n",
    "    if args.cuda:\n",
    "        starttime = datetime.datetime.now()\n",
    "        model.cuda()\n",
    "        endtime = datetime.datetime.now()\n",
    "        time_cost = (endtime - starttime).seconds\n",
    "        if args.debug: print(\"move model to gpu takes: \", time_cost, \"seconds\")\n",
    "            \n",
    "    # all workers owns the same init values\n",
    "#     init_wei = ray.get(ps.pull_weights.remote())\n",
    "#     model.load_state_dict(init_wei)\n",
    "    \n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(),\n",
    "                      lr=args.lr,\n",
    "                      momentum=args.momentum,\n",
    "                      weight_decay=args.weight_decay)\n",
    "    \n",
    "            \n",
    "    if args.refine:\n",
    "        if os.path.isfile(args.refine):\n",
    "            print('found pruned ckpt')\n",
    "            checkpoint = torch.load(args.refine)\n",
    "            model = models.__dict__['resnet'](dataset='cifar10', depth=args.depth, cfg=checkpoint['cfg'])\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer = optim.SGD(model.parameters(),\n",
    "                      lr=args.lr,\n",
    "                      momentum=args.momentum,\n",
    "                      weight_decay=args.weight_decay)\n",
    "            if args.resume:\n",
    "                print(\"args.resume filled! \")\n",
    "                checkpoint = torch.load(args.resume)\n",
    "                model.load_state_dict(checkpoint['state_dict'])\n",
    "                local_step = int(checkpoint['global_step'] / args.num_workers)\n",
    "                print(\"local_step=\",local_step)\n",
    "                args.start_epoch = checkpoint['epoch']\n",
    "                if 'optimizer' in checkpoint:\n",
    "                    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "                print(\"worker #\",worker_index,\"resumes from local_step: \",local_step)\n",
    "                if 'epoch' in checkpoint:\n",
    "                    args.start_epoch = checkpoint['epoch'] \n",
    "    elif args.resume:\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        local_step = int(checkpoint['global_step'] / args.num_workers)\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(\"worker #\",worker_index,\"resumes from local_step: \",local_step)\n",
    "        if 'epoch' in checkpoint:\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            \n",
    "    \n",
    "    wk_writer = SummaryWriter(os.path.join(os.getcwd(),args.tb_path,('wk_'+str(worker_index))))\n",
    "    print(\"worker #\",worker_index,\" is online\")\n",
    "    \n",
    "#     if local_step in [5500,7000]:\n",
    "#     for param_group in optimizer.param_groups:\n",
    "#         print(\"previous lr: \",param_group['lr'])\n",
    "#         param_group['lr'] = 0.001\n",
    "#         print(\"new lr: \",param_group['lr'])\n",
    "\n",
    "    \n",
    "    for epoch in range(args.start_epoch,args.epochs):\n",
    "        avg_loss = 0.\n",
    "        train_correct = 0.\n",
    "        for batch_idx,(data,target) in enumerate(train_loader):\n",
    "            if args.cuda:\n",
    "                starttime = datetime.datetime.now()\n",
    "                data,target = data.cuda(),target.cuda()\n",
    "                mid = datetime.datetime.now()\n",
    "                if args.debug: print(\"move data to gpu takes: \", (mid - starttime).seconds, \"seconds\")\n",
    "                model.cuda()\n",
    "                endtime = datetime.datetime.now()\n",
    "                time_cost = (endtime - starttime).seconds\n",
    "                if args.debug: print(\"move model to gpu takes: \", time_cost, \"seconds\")\n",
    "                \n",
    "            while(local_step - ray.get(ps.get_stalness.remote()) > args.stalness_limit):\n",
    "                sleep(1)\n",
    "            \n",
    "\n",
    "            # Get all weights from the parameter server.\n",
    "            if args.debug: print(\"the \",worker_index,\" pulls wei from ps.\")\n",
    "            init_wei = ray.get(ps.pull_weights.remote())\n",
    "            model.load_state_dict(init_wei)\n",
    "            model.cpu()\n",
    "#           # This doesn't contain BN running mean and var\n",
    "#             old_tensors = copy.deepcopy([p.data for p in model.parameters()]) \n",
    "\n",
    "            old_tensors = copy.deepcopy([param.data for name, param in model.state_dict().items()])\n",
    "\n",
    "            model.cuda()\n",
    "#             if args.debug: print(\"the \",worker_index,\" loaded the latest wei from ps.\")\n",
    "#             model.cpu()    \n",
    "#             wk_non_bns = [param.data for name,param in model.named_parameters() if 'bn' not in name]\n",
    "#             wk_bns = [param.data for name,param in model.named_parameters() if 'bn' in name]\n",
    "#             model.cuda()\n",
    "#             old_tensors_non_bns = copy.deepcopy(wk_non_bns)\n",
    "#             old_tensors_bns = copy.deepcopy(wk_bns)\n",
    "                \n",
    "#             # Get only non-bn weights from the parameter server.\n",
    "#             ps_non_bns = ray.get(ps.pull_non_bn_weights.remote())\n",
    "#             print(worker_index,\"pulled non bns from ps\")\n",
    "#             assert len(ps_non_bns) == len(wk_non_bns)\n",
    "#             for i in range(len(ps_non_bns)):\n",
    "#                 if i == len(ps_non_bns) / 2:\n",
    "#                     print(worker_index,\"is in the middle of updating non bns\")\n",
    "#                 wk_non_bns[i] = ps_non_bns[i]\n",
    "#             print(worker_index,\"updated non bns from ps and is going to pull bns \")\n",
    "            \n",
    "#             # Get only bn weights from the parameter server.\n",
    "#             ps_bns = ray.get(ps.pull_bn_weights.remote())\n",
    "#             print(worker_index,\"pulled bns from ps\")\n",
    "#             assert len(ps_bns) == len(wk_bns)\n",
    "#             for i in range(len(ps_bns)):\n",
    "#                 if i == len(ps_bns) / 2 :\n",
    "#                     print(worker_index, \"is in the middle of updating bns\")\n",
    "#                 wk_bns[i] = ps_bns[i]\n",
    "#             print(worker_index,\"updated bns from ps\")\n",
    "\n",
    "                \n",
    "            # Compute an update and push it to the parameter server.        \n",
    "            data, target = Variable(data), Variable(target)\n",
    "            optimizer.zero_grad()\n",
    "            if args.debug: print(worker_index,' is generating output')\n",
    "            output = model(data)\n",
    "            if args.debug: print(worker_index,' generated output done and going to calculate loss')\n",
    "            loss = F.cross_entropy(output,target)\n",
    "            avg_loss += loss\n",
    "            pred = output.data.max(1,keepdim=True)[1]\n",
    "            batch_correct = pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "            train_correct += batch_correct\n",
    "            if args.debug: print(worker_index,' calculated loss and going to bp')\n",
    "            loss.backward()\n",
    "            if args.debug: print(worker_index,' bp done')\n",
    "            \n",
    "            if(args.sr):\n",
    "                # additional subgradient descent on the sparsity-induced penalty term\n",
    "                for m in model.modules():\n",
    "                    if isinstance(m, nn.BatchNorm2d):\n",
    "                        m.weight.grad.data.add_(args.s*torch.sign(m.weight.data))  # L1\n",
    "\n",
    "            \n",
    "            # calculate difference for this iteration\n",
    "            optimizer.step()\n",
    "            model.cpu()\n",
    "#           # This doesn't contain BN running mean and var            \n",
    "#             new_tensors = [p.data for p in model.parameters()]\n",
    "            new_tensors = [param.data for name, param in model.state_dict().items()]\n",
    "\n",
    "            iter_diff = [(old_tensor - new_tensor)/args.num_workers for (old_tensor, new_tensor) in zip(old_tensors,new_tensors)]\n",
    "            model.cuda()\n",
    "            # print(\"passing sizes: \",sys.getsizeof(iter_diff))\n",
    "#             ps.apply_gradients.remote(iter_diff,worker_index,epoch)\n",
    "            ps.apply_gradients_with_running_bn.remote(iter_diff,worker_index,epoch)\n",
    "#             else:\n",
    "#                 old_tensors_non_bns = copy.deepcopy([param.data for name,param in model.named_parameters() if 'bn' not in name])\n",
    "#                 optimizer.step()\n",
    "#                 wk_non_bns = [param.data for name,param in model.named_parameters() if 'bn' not in name]\n",
    "#                 iter_diff_non_bns = [(old_tensor_non_bns - new_tensor_non_bns)/args.num_workers for (old_tensor_non_bns, new_tensor_non_bns) in zip(old_tensors_non_bns,wk_non_bns)]\n",
    "#                 ps.apply_gradients_non_bns.remote(iter_diff_non_bns,worker_index,epoch)\n",
    "        \n",
    "\n",
    "                \n",
    "            \n",
    "#             if worker_index == 0:\n",
    "# #                 print(worker_index, \"passing all params\")\n",
    "#                 # calculate difference for this iteration\n",
    "#                 old_tensors = copy.deepcopy([p.data for p in model.parameters()])    \n",
    "#                 optimizer.step()\n",
    "#                 new_tensors = [p.data for p in model.parameters()]\n",
    "#                 iter_diff = [(old_tensor - new_tensor)/args.num_workers for (old_tensor, new_tensor) in zip(old_tensors,new_tensors)]\n",
    "#                 # print(\"passing sizes: \",sys.getsizeof(iter_diff))\n",
    "#                 ps.apply_gradients.remote(iter_diff,worker_index,epoch)\n",
    "#             else:\n",
    "# #                 print(worker_index, \"passing non bn params\")\n",
    "#                 #calculate only non-bn parameters difference \n",
    "#                 old_tensors = copy.deepcopy([param.data for name,param in model.named_parameters() if 'bn' not in name])\n",
    "#                 optimizer.step()\n",
    "#                 new_tensors = [param.data for name,param in model.named_parameters() if 'bn' not in name]\n",
    "#                 iter_diff = [(old_tensor - new_tensor)/args.num_workers for (old_tensor, new_tensor) in zip(old_tensors,new_tensors)]\n",
    "#                 # print(\"passing sizes: \",sys.getsizeof(iter_diff))\n",
    "#                 ps.apply_gradients.remote(iter_diff,worker_index,epoch)\n",
    "\n",
    "#             # calculate bns and non_bns parameters difference\n",
    "#             optimizer.step()\n",
    "#             model.cpu()\n",
    "#             new_tensors_non_bns = [param.data for name,param in model.named_parameters() if 'bn' not in name]\n",
    "#             new_tensors_bns = [param.data for name,param in model.named_parameters() if 'bn' in name]\n",
    "#             model.cuda()\n",
    "#             iter_diff_non_bns = [(old_tensor_non_bns - new_tensor_non_bns)/args.num_workers for (old_tensor_non_bns, new_tensor_non_bns) in zip(old_tensors_non_bns,new_tensors_non_bns)]\n",
    "#             iter_diff_bns = [(old_tensor_bns - new_tensor_bns)/args.num_workers for (old_tensor_bns, new_tensor_bns) in zip(old_tensors_bns,new_tensors_bns)]\n",
    "#             # print(\"passing sizes: \",sys.getsizeof(iter_diff))\n",
    "#             ps.apply_gradients_non_bns.remote(iter_diff_non_bns,worker_index,epoch)\n",
    "#             print(worker_index, \"pushed non bns to ps and is going to push bns to ps\")\n",
    "#             ps.apply_gradients_partical_bns.remote(iter_diff_bns,worker_index,epoch)\n",
    "#             print(worker_index, \"pushed bns to ps\")\n",
    "            \n",
    "            \n",
    "#             # aggregate and sync bn parameters    \n",
    "#             if sync_bns_flag:\n",
    "#                 print(\"SYNC BNS: worker #\",worker_index,\" is checking to pull bns from ps \")\n",
    "#                 if ray.get(ps.get_bns_ready.remote()):\n",
    "#                     print(\"SYNC BNS: worker #\",worker_index,\" is pulling bns from ps \")\n",
    "#                     ps_wei = ray.get(ps.pull_weights.remote())\n",
    "#                     model.load_state_dict(ps_wei)\n",
    "#                     sync_bns_flag = False\n",
    "#             if local_step % args.sync_bns == 0:\n",
    "#                 sync_bns_flag = True\n",
    "#                 model.cpu()\n",
    "#                 if args.debug: print(\"SYNC BNS: goint to aggregate bns\")\n",
    "#                 wk_bns = [param.data for name,param in model.named_parameters() if 'bn' in name]\n",
    "#                 bns_sync = ray.get(ps.aggregate_bns.remote(wk_bns,worker_index))\n",
    "#                 print(\"SYNC BNS: bns of worker #\",worker_index,\" have been pushed\")\n",
    "                \n",
    "#                 if any(bns_sync) == False: # when all workers have pushed their own bns parameters to ps\n",
    "#                     print(\"SYNC BNS: worker #\",worker_index,\" is last worker pushed its bns\")\n",
    "#                     ps_wei = ray.get(ps.pull_weights.remote())\n",
    "#                     model.load_state_dict(ps_wei)\n",
    "#                     sync_bns_flag = False\n",
    "#                 if args.cuda: model.cuda()\n",
    "            \n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "#             if local_step in [5500,7000]:\n",
    "#                 for param_group in optimizer.param_groups:\n",
    "#                     print(\"previous lr: \",param_group['lr'])\n",
    "#                     param_group['lr'] *= 0.1\n",
    "#                     print(\"new lr: \",param_group['lr'])\n",
    "                \n",
    "            local_step += 1\n",
    "\n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                print('The {} worker, Train Epoch: {} [{}/{} ({:.1f}%)]\\tLoss: {:.6f}'.format(\n",
    "                worker_index, epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data))\n",
    "                \n",
    "                for name,param in model.named_parameters():\n",
    "                    wk_writer.add_histogram(name, param, local_step)\n",
    "\n",
    "                wk_writer.add_scalar(\"Loss/worker_train\",loss,local_step)\n",
    "                wk_writer.add_scalar(\"Accuracy/worker_train\",batch_correct.float()/len(data),local_step)\n",
    "                \n",
    "        print(\"The {} worker finished its {} epoch with loss: {} and accuracy: {}\".format(\n",
    "            worker_index,\n",
    "            epoch,\n",
    "            avg_loss / float(len(train_loader.dataset)),\n",
    "            train_correct.float() / float(len(train_loader.dataset))\n",
    "        ))\n",
    "    print(\"worker #\",worker_index,\" has finished its job, going offline\")\n",
    "    ps.set_finished.remote(worker_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Distributed SSP CIFAR-10 Restnet train with network slimming')\n",
    "parser.add_argument('--ray-master',type=str,default='127.0.0.1')\n",
    "parser.add_argument('--redis-port',type=str,default='6379')\n",
    "parser.add_argument('--batch-size',type=int,default=64)\n",
    "parser.add_argument('--test-batch-size', type=int, default=64)\n",
    "parser.add_argument('--epochs', type=int, default=3)\n",
    "parser.add_argument('--start-epoch', default=0, type=int)\n",
    "parser.add_argument('--lr', type=float, default=0.1)\n",
    "parser.add_argument('--momentum', type=float, default=0.9)\n",
    "parser.add_argument('--weight-decay', '--wd', default=1e-4, type=float)\n",
    "parser.add_argument('--resume', default=None, type=str) \n",
    "parser.add_argument('--refine', default=None, type=str) \n",
    "parser.add_argument('--no-cuda', action='store_true', default=False)\n",
    "parser.add_argument('--save', default='./logs', type=str)\n",
    "parser.add_argument('--depth', default=164, type=int)\n",
    "parser.add_argument('--tb-path', default='./logs', type=str)\n",
    "parser.add_argument('--log-interval', type=int, default=100)\n",
    "parser.add_argument('--num-workers',type=int,default=1)\n",
    "parser.add_argument('--stalness-limit',type=int,default=5)\n",
    "parser.add_argument('--debug',action='store_true',default=False)\n",
    "parser.add_argument('--sync-bns',type=int, default=194)\n",
    "parser.add_argument('--sparsity-regularization', '-sr', dest='sr', action='store_true',\n",
    "                    help='train with channel sparsity regularization')\n",
    "parser.add_argument('--s', type=float, default=0.0001,\n",
    "                    help='scale sparse rate (default: 0.0001)')\n",
    "\n",
    "\n",
    "args = parser.parse_args(args=['--num-workers=3','--tb-path=/userhome/34/gyu/logs_sr/3wk_p1/train34/',\n",
    "                               '--save=/userhome/34/gyu/logs_sr/3wk_p1/train34/',\n",
    "                               '--epochs=10',\n",
    "                               '--refine=/userhome/34/gyu/logs_sr/3wk_p1/train24/checkpoint.pth.tar',\n",
    "                               '--lr=0.001',\n",
    "                               '-sr','--s=0.00001'])\n",
    "\n",
    "# '--resume=/userhome/34/gyu/logs_sr/checkpoint.pth.tar'\n",
    "# '--tb-path=logs_no_bns','--save=logs_no_bns'\n",
    "# '-sr','--s=0.00001'\n",
    "\n",
    "# '--refine=/userhome/34/gyu/logs_sr/3wk_p1/prune_1st/pruned.pth.tar',\n",
    "# '--resume=/userhome/34/gyu/logs_sr/3wk_p1/prune_1st/refine1/checkpoint.pth.tar',\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ray.is_initialized():\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '10.21.5.171',\n",
       " 'redis_address': '10.21.5.171:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2019-12-30_15-42-27_520221_8838/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2019-12-30_15-42-27_520221_8838/sockets/raylet',\n",
       " 'webui_url': 'http://10.21.5.171:8080/?token=f97b60ba77ae3f54cbf67e4af32807b34f1cd5a2730770f6',\n",
       " 'session_dir': '/tmp/ray/session_2019-12-30_15-42-27_520221_8838'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(address=args.ray_master+':'+args.redis_port)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m found pruned ckpt\n"
     ]
    }
   ],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "\n",
    "test_loader = generate_test_loader(args.test_batch_size)\n",
    "train_loaders = [generate_train_loader(args.batch_size,kwargs) for _ in range(args.num_workers)]\n",
    "\n",
    "resume_from_ckpt = args.resume if (args.resume and os.path.isfile(args.resume)) else None\n",
    "\n",
    "ps = ParameterServer.remote(args,test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m found pruned ckpt\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m worker # 0  is online\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 0 [0/50000 (0.0%)]\tLoss: 0.065305\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m found pruned ckpt\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m worker # 1  is online\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 0 [0/50000 (0.0%)]\tLoss: 0.095775\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m found pruned ckpt\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m worker # 2  is online\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 0 [0/50000 (0.0%)]\tLoss: 0.122959\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 0 [6400/50000 (12.8%)]\tLoss: 0.064800\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 0 [6400/50000 (12.8%)]\tLoss: 0.034106\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 0 [6400/50000 (12.8%)]\tLoss: 0.081878\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 0 [12800/50000 (25.6%)]\tLoss: 0.036039\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 0 [12800/50000 (25.6%)]\tLoss: 0.069174\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 0 [12800/50000 (25.6%)]\tLoss: 0.144716\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 0 [19200/50000 (38.4%)]\tLoss: 0.121668\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 0 [19200/50000 (38.4%)]\tLoss: 0.145853\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 0 [19200/50000 (38.4%)]\tLoss: 0.042919\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 0 [25600/50000 (51.2%)]\tLoss: 0.219482\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 0 [25600/50000 (51.2%)]\tLoss: 0.062220\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 0 [25600/50000 (51.2%)]\tLoss: 0.084168\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 0 [32000/50000 (63.9%)]\tLoss: 0.180793\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 0 [32000/50000 (63.9%)]\tLoss: 0.106141\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 0 [32000/50000 (63.9%)]\tLoss: 0.074235\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 0 [38400/50000 (76.7%)]\tLoss: 0.139650\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 0 [38400/50000 (76.7%)]\tLoss: 0.161748\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 0 [38400/50000 (76.7%)]\tLoss: 0.141191\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 0 [44800/50000 (89.5%)]\tLoss: 0.103961\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 0 [44800/50000 (89.5%)]\tLoss: 0.088628\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 0 [44800/50000 (89.5%)]\tLoss: 0.102849\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker finished its 0 epoch with loss: 0.0016215710202232003 and accuracy: 0.9642999768257141\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker finished its 0 epoch with loss: 0.0015970575623214245 and accuracy: 0.9648600220680237\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 1 [0/50000 (0.0%)]\tLoss: 0.038399\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 1 [0/50000 (0.0%)]\tLoss: 0.094535\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m global_step:  2346  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m going to evaluate\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m pulled weights\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m loaded weights\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m length of the test_loader dataset is :  10000\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m /userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py:235: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m /userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m   warnings.warn(warning.format(ret))\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  20\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  40\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  60\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  80\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  100\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  120\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  140\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m Test set: Average loss: 0.3596, Accuracy: 8979/10000 (0.8979)\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker finished its 0 epoch with loss: 0.00162625836674124 and accuracy: 0.9645199775695801\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 1 [0/50000 (0.0%)]\tLoss: 0.054809\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 1 [6400/50000 (12.8%)]\tLoss: 0.038222\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 1 [6400/50000 (12.8%)]\tLoss: 0.101866\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 1 [6400/50000 (12.8%)]\tLoss: 0.107089\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 1 [12800/50000 (25.6%)]\tLoss: 0.090461\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 1 [12800/50000 (25.6%)]\tLoss: 0.106647\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 1 [12800/50000 (25.6%)]\tLoss: 0.026840\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 1 [19200/50000 (38.4%)]\tLoss: 0.085412\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 1 [19200/50000 (38.4%)]\tLoss: 0.064310\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 1 [19200/50000 (38.4%)]\tLoss: 0.070659\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 1 [25600/50000 (51.2%)]\tLoss: 0.175652\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 1 [25600/50000 (51.2%)]\tLoss: 0.095790\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 1 [25600/50000 (51.2%)]\tLoss: 0.107597\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 1 [32000/50000 (63.9%)]\tLoss: 0.091821\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 1 [32000/50000 (63.9%)]\tLoss: 0.137912\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 1 [32000/50000 (63.9%)]\tLoss: 0.115053\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 1 [38400/50000 (76.7%)]\tLoss: 0.081277\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 1 [38400/50000 (76.7%)]\tLoss: 0.033818\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 1 [38400/50000 (76.7%)]\tLoss: 0.108551\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 1 [44800/50000 (89.5%)]\tLoss: 0.092488\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 1 [44800/50000 (89.5%)]\tLoss: 0.037038\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 1 [44800/50000 (89.5%)]\tLoss: 0.067046\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker finished its 1 epoch with loss: 0.0015706598060205579 and accuracy: 0.9654800295829773\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker finished its 1 epoch with loss: 0.0015433385269716382 and accuracy: 0.96670001745224\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 2 [0/50000 (0.0%)]\tLoss: 0.115980\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 2 [0/50000 (0.0%)]\tLoss: 0.062265\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m global_step:  4692  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m going to evaluate\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m pulled weights\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m loaded weights\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m length of the test_loader dataset is :  10000\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  20\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  40\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  60\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  80\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  100\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  120\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  140\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m Test set: Average loss: 0.3675, Accuracy: 8959/10000 (0.8959)\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker finished its 1 epoch with loss: 0.001549417502246797 and accuracy: 0.966759979724884\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 2 [0/50000 (0.0%)]\tLoss: 0.062634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 2 [6400/50000 (12.8%)]\tLoss: 0.155454\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 2 [6400/50000 (12.8%)]\tLoss: 0.091168\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 2 [6400/50000 (12.8%)]\tLoss: 0.145853\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 2 [12800/50000 (25.6%)]\tLoss: 0.047951\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 2 [12800/50000 (25.6%)]\tLoss: 0.082478\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 2 [12800/50000 (25.6%)]\tLoss: 0.079165\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 2 [19200/50000 (38.4%)]\tLoss: 0.034892\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 2 [19200/50000 (38.4%)]\tLoss: 0.095746\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 2 [19200/50000 (38.4%)]\tLoss: 0.107943\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 2 [25600/50000 (51.2%)]\tLoss: 0.222652\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 2 [25600/50000 (51.2%)]\tLoss: 0.103949\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 2 [25600/50000 (51.2%)]\tLoss: 0.100242\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 2 [32000/50000 (63.9%)]\tLoss: 0.073616\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 2 [32000/50000 (63.9%)]\tLoss: 0.045695\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 2 [32000/50000 (63.9%)]\tLoss: 0.098068\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 2 [38400/50000 (76.7%)]\tLoss: 0.086649\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 2 [38400/50000 (76.7%)]\tLoss: 0.046588\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 2 [38400/50000 (76.7%)]\tLoss: 0.160561\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 2 [44800/50000 (89.5%)]\tLoss: 0.053122\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 2 [44800/50000 (89.5%)]\tLoss: 0.038105\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 2 [44800/50000 (89.5%)]\tLoss: 0.027344\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker finished its 2 epoch with loss: 0.0014956232625991106 and accuracy: 0.9674400091171265\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker finished its 2 epoch with loss: 0.0015332846669480205 and accuracy: 0.9666600227355957\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 3 [0/50000 (0.0%)]\tLoss: 0.139474\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 3 [0/50000 (0.0%)]\tLoss: 0.188130\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m global_step:  7038  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m going to evaluate\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m pulled weights\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m loaded weights\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m length of the test_loader dataset is :  10000\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  20\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  40\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  60\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  80\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  100\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  120\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  140\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m Test set: Average loss: 0.3682, Accuracy: 8982/10000 (0.8982)\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker finished its 2 epoch with loss: 0.0015374826034530997 and accuracy: 0.9662600159645081\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 3 [0/50000 (0.0%)]\tLoss: 0.103833\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 3 [6400/50000 (12.8%)]\tLoss: 0.103704\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 3 [6400/50000 (12.8%)]\tLoss: 0.105304\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 3 [6400/50000 (12.8%)]\tLoss: 0.060798\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 3 [12800/50000 (25.6%)]\tLoss: 0.086842\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 3 [12800/50000 (25.6%)]\tLoss: 0.091317\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 3 [12800/50000 (25.6%)]\tLoss: 0.154628\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 3 [19200/50000 (38.4%)]\tLoss: 0.084044\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 3 [19200/50000 (38.4%)]\tLoss: 0.036237\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 3 [19200/50000 (38.4%)]\tLoss: 0.166153\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 3 [25600/50000 (51.2%)]\tLoss: 0.100758\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 3 [25600/50000 (51.2%)]\tLoss: 0.057919\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 3 [25600/50000 (51.2%)]\tLoss: 0.047156\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 3 [32000/50000 (63.9%)]\tLoss: 0.110811\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 3 [32000/50000 (63.9%)]\tLoss: 0.088434\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 3 [32000/50000 (63.9%)]\tLoss: 0.102909\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 3 [38400/50000 (76.7%)]\tLoss: 0.240548\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 3 [38400/50000 (76.7%)]\tLoss: 0.066633\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 3 [38400/50000 (76.7%)]\tLoss: 0.104970\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 3 [44800/50000 (89.5%)]\tLoss: 0.098974\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 3 [44800/50000 (89.5%)]\tLoss: 0.049651\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 3 [44800/50000 (89.5%)]\tLoss: 0.179998\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker finished its 3 epoch with loss: 0.0015285883564502 and accuracy: 0.9668999910354614\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker finished its 3 epoch with loss: 0.0014891976024955511 and accuracy: 0.9681000113487244\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 4 [0/50000 (0.0%)]\tLoss: 0.143900\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 4 [0/50000 (0.0%)]\tLoss: 0.110702\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m global_step:  9384  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m going to evaluate\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m pulled weights\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m loaded weights\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m length of the test_loader dataset is :  10000\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  20\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  40\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  60\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  80\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  100\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  120\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  140\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m Test set: Average loss: 0.3684, Accuracy: 8984/10000 (0.8984)\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker finished its 3 epoch with loss: 0.0014541487907990813 and accuracy: 0.968720018863678\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 4 [0/50000 (0.0%)]\tLoss: 0.081757\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 4 [6400/50000 (12.8%)]\tLoss: 0.029364\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 4 [6400/50000 (12.8%)]\tLoss: 0.103796\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 4 [6400/50000 (12.8%)]\tLoss: 0.154435\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 4 [12800/50000 (25.6%)]\tLoss: 0.057746\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 4 [12800/50000 (25.6%)]\tLoss: 0.113585\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 4 [12800/50000 (25.6%)]\tLoss: 0.055364\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 4 [19200/50000 (38.4%)]\tLoss: 0.061398\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 4 [19200/50000 (38.4%)]\tLoss: 0.179772\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 4 [19200/50000 (38.4%)]\tLoss: 0.148404\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 4 [25600/50000 (51.2%)]\tLoss: 0.198175\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 4 [25600/50000 (51.2%)]\tLoss: 0.116368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 4 [25600/50000 (51.2%)]\tLoss: 0.094184\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 4 [32000/50000 (63.9%)]\tLoss: 0.110086\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 4 [32000/50000 (63.9%)]\tLoss: 0.126409\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 4 [32000/50000 (63.9%)]\tLoss: 0.156977\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 4 [38400/50000 (76.7%)]\tLoss: 0.219556\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 4 [38400/50000 (76.7%)]\tLoss: 0.084912\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 4 [38400/50000 (76.7%)]\tLoss: 0.139367\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 4 [44800/50000 (89.5%)]\tLoss: 0.057243\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 4 [44800/50000 (89.5%)]\tLoss: 0.093523\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 4 [44800/50000 (89.5%)]\tLoss: 0.033192\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker finished its 4 epoch with loss: 0.0014382307417690754 and accuracy: 0.968999981880188\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker finished its 4 epoch with loss: 0.0014279981842264533 and accuracy: 0.9696199893951416\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 5 [0/50000 (0.0%)]\tLoss: 0.085515\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 5 [0/50000 (0.0%)]\tLoss: 0.040398\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m global_step:  11730  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m going to evaluate\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m pulled weights\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m loaded weights\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m length of the test_loader dataset is :  10000\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  20\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  40\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  60\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  80\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  100\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  120\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  140\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m Test set: Average loss: 0.3735, Accuracy: 8965/10000 (0.8965)\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker finished its 4 epoch with loss: 0.0014829941792413592 and accuracy: 0.967739999294281\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 5 [0/50000 (0.0%)]\tLoss: 0.083931\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 5 [6400/50000 (12.8%)]\tLoss: 0.171000\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 5 [6400/50000 (12.8%)]\tLoss: 0.060704\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 5 [6400/50000 (12.8%)]\tLoss: 0.101053\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 5 [12800/50000 (25.6%)]\tLoss: 0.078364\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 5 [12800/50000 (25.6%)]\tLoss: 0.100039\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 5 [12800/50000 (25.6%)]\tLoss: 0.030310\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 5 [19200/50000 (38.4%)]\tLoss: 0.027895\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 5 [19200/50000 (38.4%)]\tLoss: 0.056461\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 5 [19200/50000 (38.4%)]\tLoss: 0.035479\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 5 [25600/50000 (51.2%)]\tLoss: 0.018695\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 5 [25600/50000 (51.2%)]\tLoss: 0.066317\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 5 [25600/50000 (51.2%)]\tLoss: 0.054263\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 5 [32000/50000 (63.9%)]\tLoss: 0.034506\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 5 [32000/50000 (63.9%)]\tLoss: 0.099804\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 5 [32000/50000 (63.9%)]\tLoss: 0.105428\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 5 [38400/50000 (76.7%)]\tLoss: 0.044201\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 5 [38400/50000 (76.7%)]\tLoss: 0.156109\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 5 [38400/50000 (76.7%)]\tLoss: 0.135034\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 5 [44800/50000 (89.5%)]\tLoss: 0.128000\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 5 [44800/50000 (89.5%)]\tLoss: 0.073726\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 5 [44800/50000 (89.5%)]\tLoss: 0.036717\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker finished its 5 epoch with loss: 0.0013970352010801435 and accuracy: 0.9699599742889404\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker finished its 5 epoch with loss: 0.001459441613405943 and accuracy: 0.9688000082969666\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 6 [0/50000 (0.0%)]\tLoss: 0.066935\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 6 [0/50000 (0.0%)]\tLoss: 0.035749\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m global_step:  14076  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m going to evaluate\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m pulled weights\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m loaded weights\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m length of the test_loader dataset is :  10000\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  20\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  40\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  60\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  80\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  100\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  120\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  140\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m Test set: Average loss: 0.3725, Accuracy: 8964/10000 (0.8964)\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker finished its 5 epoch with loss: 0.001438339240849018 and accuracy: 0.9690200090408325\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 6 [0/50000 (0.0%)]\tLoss: 0.025561\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 6 [6400/50000 (12.8%)]\tLoss: 0.062525\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 6 [6400/50000 (12.8%)]\tLoss: 0.030886\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 6 [6400/50000 (12.8%)]\tLoss: 0.176341\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 6 [12800/50000 (25.6%)]\tLoss: 0.061074\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 6 [12800/50000 (25.6%)]\tLoss: 0.087505\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 6 [12800/50000 (25.6%)]\tLoss: 0.086629\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 6 [19200/50000 (38.4%)]\tLoss: 0.040596\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 6 [19200/50000 (38.4%)]\tLoss: 0.148699\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 6 [19200/50000 (38.4%)]\tLoss: 0.053032\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 6 [25600/50000 (51.2%)]\tLoss: 0.125531\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 6 [25600/50000 (51.2%)]\tLoss: 0.131621\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 6 [25600/50000 (51.2%)]\tLoss: 0.084418\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 6 [32000/50000 (63.9%)]\tLoss: 0.153506\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 6 [32000/50000 (63.9%)]\tLoss: 0.029024\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 6 [32000/50000 (63.9%)]\tLoss: 0.064911\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 6 [38400/50000 (76.7%)]\tLoss: 0.079059\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 6 [38400/50000 (76.7%)]\tLoss: 0.107841\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 6 [38400/50000 (76.7%)]\tLoss: 0.067057\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 6 [44800/50000 (89.5%)]\tLoss: 0.053902\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 6 [44800/50000 (89.5%)]\tLoss: 0.057347\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 6 [44800/50000 (89.5%)]\tLoss: 0.077323\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker finished its 6 epoch with loss: 0.0014205776387825608 and accuracy: 0.9692800045013428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker finished its 6 epoch with loss: 0.0014155225362628698 and accuracy: 0.9690399765968323\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 7 [0/50000 (0.0%)]\tLoss: 0.037290\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 7 [0/50000 (0.0%)]\tLoss: 0.094349\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m global_step:  16422  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m going to evaluate\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m pulled weights\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m loaded weights\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m length of the test_loader dataset is :  10000\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  20\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  40\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  60\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  80\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  100\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  120\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  140\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m Test set: Average loss: 0.3747, Accuracy: 8989/10000 (0.8989)\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker finished its 6 epoch with loss: 0.0013929714914411306 and accuracy: 0.969219982624054\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 7 [0/50000 (0.0%)]\tLoss: 0.055163\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 7 [6400/50000 (12.8%)]\tLoss: 0.189632\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 7 [6400/50000 (12.8%)]\tLoss: 0.050933\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 7 [6400/50000 (12.8%)]\tLoss: 0.046232\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 7 [12800/50000 (25.6%)]\tLoss: 0.045992\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 7 [12800/50000 (25.6%)]\tLoss: 0.071079\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 7 [12800/50000 (25.6%)]\tLoss: 0.124379\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 7 [19200/50000 (38.4%)]\tLoss: 0.047083\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 7 [19200/50000 (38.4%)]\tLoss: 0.066771\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 7 [19200/50000 (38.4%)]\tLoss: 0.089928\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 7 [25600/50000 (51.2%)]\tLoss: 0.053773\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 7 [25600/50000 (51.2%)]\tLoss: 0.134392\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 7 [25600/50000 (51.2%)]\tLoss: 0.133460\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 7 [38400/50000 (76.7%)]\tLoss: 0.072504\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 7 [38400/50000 (76.7%)]\tLoss: 0.086913\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 7 [38400/50000 (76.7%)]\tLoss: 0.032644\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 7 [44800/50000 (89.5%)]\tLoss: 0.195823\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 7 [44800/50000 (89.5%)]\tLoss: 0.093718\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 7 [44800/50000 (89.5%)]\tLoss: 0.151259\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker finished its 7 epoch with loss: 0.0013624359853565693 and accuracy: 0.9700400233268738\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker finished its 7 epoch with loss: 0.001353952451609075 and accuracy: 0.97079998254776\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 8 [0/50000 (0.0%)]\tLoss: 0.048042\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 8 [0/50000 (0.0%)]\tLoss: 0.039467\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m global_step:  18768  and prepare evaluate\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m going to evaluate\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m pulled weights\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m loaded weights\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m length of the test_loader dataset is :  10000\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  20\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  40\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  60\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  80\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  100\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  120\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m in eval, the batch is:  140\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m Test set: Average loss: 0.3797, Accuracy: 8988/10000 (0.8988)\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker finished its 7 epoch with loss: 0.001354063511826098 and accuracy: 0.9708399772644043\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 8 [0/50000 (0.0%)]\tLoss: 0.077949\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 8 [6400/50000 (12.8%)]\tLoss: 0.070428\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 8 [6400/50000 (12.8%)]\tLoss: 0.086265\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 8 [6400/50000 (12.8%)]\tLoss: 0.172364\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 8 [12800/50000 (25.6%)]\tLoss: 0.046790\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 8 [12800/50000 (25.6%)]\tLoss: 0.102945\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m The 2 worker, Train Epoch: 8 [12800/50000 (25.6%)]\tLoss: 0.149594\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m The 0 worker, Train Epoch: 8 [19200/50000 (38.4%)]\tLoss: 0.074643\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m The 1 worker, Train Epoch: 8 [19200/50000 (38.4%)]\tLoss: 0.052679\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.173)\u001b[0m F0102 20:54:09.317267 11567 node_manager.cc:481]  Check failed: client_id != gcs_client_->client_table().GetLocalClientId() Exiting because this node manager has mistakenly been marked dead by the monitor.\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.173)\u001b[0m *** Check failure stack trace: ***\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.173)\u001b[0m     @           0x6f8d1a  google::LogMessage::Fail()\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.173)\u001b[0m     @           0x6fa103  google::LogMessage::SendToLog()\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.173)\u001b[0m     @           0x6f8a42  google::LogMessage::Flush()\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.173)\u001b[0m     @           0x6f8c31  google::LogMessage::~LogMessage()\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.173)\u001b[0m     @           0x52b0d2  ray::RayLog::~RayLog()\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.173)\u001b[0m     @           0x466752  ray::raylet::NodeManager::ClientRemoved()\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.173)\u001b[0m     @           0x4b790e  ray::gcs::ClientTable::HandleNotification()\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.173)\u001b[0m     @           0x4d328b  _ZNSt17_Function_handlerIFvPN3ray3gcs14RedisGcsClientERKNS0_8ClientIDERKSt6vectorINS0_3rpc11GcsNodeInfoESaIS9_EEEZZNS1_11ClientTable7ConnectERKS9_ENKUlS3_RKNS0_8UniqueIDESH_E_clES3_SK_SH_EUlS3_SK_SD_E_E9_M_invokeERKSt9_Any_dataS3_S6_SD_\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.173)\u001b[0m     @           0x4d2876  _ZNSt17_Function_handlerIFvPN3ray3gcs14RedisGcsClientERKNS0_8ClientIDENS0_3rpc13GcsChangeModeERKSt6vectorINS7_11GcsNodeInfoESaISA_EEEZNS1_3LogIS4_SA_E9SubscribeERKNS0_5JobIDES6_RKSt8functionIFvS3_S6_SE_EERKSL_IFvS3_EEEUlS3_S6_S8_SE_E_E9_M_invokeERKSt9_Any_dataS3_S6_S8_SE_\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.173)\u001b[0m     @           0x4b5893  _ZZN3ray3gcs3LogINS_8ClientIDENS_3rpc11GcsNodeInfoEE9SubscribeERKNS_5JobIDERKS2_RKSt8functionIFvPNS0_14RedisGcsClientESA_NS3_13GcsChangeModeERKSt6vectorIS4_SaIS4_EEEERKSB_IFvSD_EEENKUlRKNS0_13CallbackReplyEE_clESU_\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.173)\u001b[0m     @           0x4dad09  ray::gcs::GlobalRedisCallback()\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.173)\u001b[0m     @           0x4dfa1b  redisProcessCallbacks"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-02 20:54:22,974\tWARNING worker.py:1779 -- The node with client ID 6033f15404166e0f6f4ae4d479026f378c1404f5 has been marked dead because the monitor has missed too many heartbeats from it.\n",
      "2020-01-02 20:54:44,127\tWARNING worker.py:1779 -- The node with client ID 0e3ba9d972bf1d63d16c9398ac45ecd6f1f3ee94 has been marked dead because the monitor has missed too many heartbeats from it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.173)\u001b[0m     @           0x4de776  RedisAsioClient::handle_read()\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.173)\u001b[0m     @           0x4dd9a8  boost::asio::detail::reactive_null_buffers_op<>::do_complete()\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.173)\u001b[0m     @           0x425bcd  boost::asio::detail::scheduler::run()\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.173)\u001b[0m     @           0x40fb1d  main\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.173)\u001b[0m     @     0x7f2ba8438b97  __libc_start_main\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.173)\u001b[0m     @           0x4207e1  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.174)\u001b[0m F0102 20:54:09.338240  5908 node_manager.cc:481]  Check failed: client_id != gcs_client_->client_table().GetLocalClientId() Exiting because this node manager has mistakenly been marked dead by the monitor.\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.174)\u001b[0m *** Check failure stack trace: ***\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.174)\u001b[0m     @           0x6f8d1a  google::LogMessage::Fail()\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.174)\u001b[0m     @           0x6fa103  google::LogMessage::SendToLog()\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.174)\u001b[0m     @           0x6f8a42  google::LogMessage::Flush()\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.174)\u001b[0m     @           0x6f8c31  google::LogMessage::~LogMessage()\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.174)\u001b[0m     @           0x52b0d2  ray::RayLog::~RayLog()\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.174)\u001b[0m     @           0x466752  ray::raylet::NodeManager::ClientRemoved()\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.174)\u001b[0m     @           0x4b790e  ray::gcs::ClientTable::HandleNotification()\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.174)\u001b[0m     @           0x4d328b  _ZNSt17_Function_handlerIFvPN3ray3gcs14RedisGcsClientERKNS0_8ClientIDERKSt6vectorINS0_3rpc11GcsNodeInfoESaIS9_EEEZZNS1_11ClientTable7ConnectERKS9_ENKUlS3_RKNS0_8UniqueIDESH_E_clES3_SK_SH_EUlS3_SK_SD_E_E9_M_invokeERKSt9_Any_dataS3_S6_SD_\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.174)\u001b[0m     @           0x4d2876  _ZNSt17_Function_handlerIFvPN3ray3gcs14RedisGcsClientERKNS0_8ClientIDENS0_3rpc13GcsChangeModeERKSt6vectorINS7_11GcsNodeInfoESaISA_EEEZNS1_3LogIS4_SA_E9SubscribeERKNS0_5JobIDES6_RKSt8functionIFvS3_S6_SE_EERKSL_IFvS3_EEEUlS3_S6_S8_SE_E_E9_M_invokeERKSt9_Any_dataS3_S6_S8_SE_\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.174)\u001b[0m     @           0x4b5893  _ZZN3ray3gcs3LogINS_8ClientIDENS_3rpc11GcsNodeInfoEE9SubscribeERKNS_5JobIDERKS2_RKSt8functionIFvPNS0_14RedisGcsClientESA_NS3_13GcsChangeModeERKSt6vectorIS4_SaIS4_EEEERKSB_IFvSD_EEENKUlRKNS0_13CallbackReplyEE_clESU_\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.174)\u001b[0m     @           0x4dad09  ray::gcs::GlobalRedisCallback()\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.174)\u001b[0m     @           0x4dfa1b  redisProcessCallbacks\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.174)\u001b[0m     @           0x4de776  RedisAsioClient::handle_read()\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.174)\u001b[0m     @           0x4dd9a8  boost::asio::detail::reactive_null_buffers_op<>::do_complete()\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.174)\u001b[0m     @           0x425bcd  boost::asio::detail::scheduler::run()\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.174)\u001b[0m     @           0x40fb1d  main\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.174)\u001b[0m     @     0x7f3d96fb7b97  __libc_start_main\n",
      "\u001b[2m\u001b[33m(pid=raylet, ip=10.21.5.174)\u001b[0m     @           0x4207e1  (unknown)\n",
      "\u001b[2m\u001b[36m(pid=5928, ip=10.21.5.174)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=5928, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 98, in <module>\n",
      "\u001b[2m\u001b[36m(pid=5928, ip=10.21.5.174)\u001b[0m     ray.worker.global_worker.main_loop()\n",
      "\u001b[2m\u001b[36m(pid=5928, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1118, in main_loop\n",
      "\u001b[2m\u001b[36m(pid=5928, ip=10.21.5.174)\u001b[0m     task = self._get_next_task_from_raylet()\n",
      "\u001b[2m\u001b[36m(pid=5928, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1101, in _get_next_task_from_raylet\n",
      "\u001b[2m\u001b[36m(pid=5928, ip=10.21.5.174)\u001b[0m     task = self.raylet_client.get_task()\n",
      "\u001b[2m\u001b[36m(pid=5928, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 291, in ray._raylet.RayletClient.get_task\n",
      "\u001b[2m\u001b[36m(pid=5928, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=5928, ip=10.21.5.174)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Raylet connection closed.\n",
      "\u001b[2m\u001b[36m(pid=5928, ip=10.21.5.174)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5928, ip=10.21.5.174)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=5928, ip=10.21.5.174)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5928, ip=10.21.5.174)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=5928, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 105, in <module>\n",
      "\u001b[2m\u001b[36m(pid=5928, ip=10.21.5.174)\u001b[0m     job_id=None)\n",
      "\u001b[2m\u001b[36m(pid=5928, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/utils.py\", line 67, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=11579, ip=10.21.5.173)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=11579, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 98, in <module>\n",
      "\u001b[2m\u001b[36m(pid=11579, ip=10.21.5.173)\u001b[0m     ray.worker.global_worker.main_loop()\n",
      "\u001b[2m\u001b[36m(pid=11579, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1118, in main_loop\n",
      "\u001b[2m\u001b[36m(pid=11579, ip=10.21.5.173)\u001b[0m     task = self._get_next_task_from_raylet()\n",
      "\u001b[2m\u001b[36m(pid=11579, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1101, in _get_next_task_from_raylet\n",
      "\u001b[2m\u001b[36m(pid=11579, ip=10.21.5.173)\u001b[0m     task = self.raylet_client.get_task()\n",
      "\u001b[2m\u001b[36m(pid=11579, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 291, in ray._raylet.RayletClient.get_task\n",
      "\u001b[2m\u001b[36m(pid=11579, ip=10.21.5.173)\u001b[0m E0102 20:54:10.147042 11668 raylet_client.cc:339] IOError: [RayletClient] Connection closed unexpectedly. [RayletClient] Failed to push profile events.\n",
      "\u001b[2m\u001b[36m(pid=11579, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=11579, ip=10.21.5.173)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Raylet connection closed.\n",
      "\u001b[2m\u001b[36m(pid=11579, ip=10.21.5.173)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11579, ip=10.21.5.173)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=11579, ip=10.21.5.173)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11579, ip=10.21.5.173)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=11579, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 105, in <module>\n",
      "\u001b[2m\u001b[36m(pid=11579, ip=10.21.5.173)\u001b[0m     job_id=None)\n",
      "\u001b[2m\u001b[36m(pid=11579, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/utils.py\", line 67, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=11579, ip=10.21.5.173)\u001b[0m     worker.raylet_client.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=11579, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 327, in ray._raylet.RayletClient.push_error\n",
      "\u001b[2m\u001b[36m(pid=11579, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=11579, ip=10.21.5.173)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.\n",
      "\u001b[2m\u001b[36m(pid=5927, ip=10.21.5.174)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=5927, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 98, in <module>\n",
      "\u001b[2m\u001b[36m(pid=5927, ip=10.21.5.174)\u001b[0m     ray.worker.global_worker.main_loop()\n",
      "\u001b[2m\u001b[36m(pid=5927, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1118, in main_loop\n",
      "\u001b[2m\u001b[36m(pid=5927, ip=10.21.5.174)\u001b[0m     task = self._get_next_task_from_raylet()\n",
      "\u001b[2m\u001b[36m(pid=5927, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1101, in _get_next_task_from_raylet\n",
      "\u001b[2m\u001b[36m(pid=5927, ip=10.21.5.174)\u001b[0m E0102 20:54:09.827307  6015 raylet_client.cc:339] IOError: [RayletClient] Connection closed unexpectedly. [RayletClient] Failed to push profile events.\n",
      "\u001b[2m\u001b[36m(pid=5927, ip=10.21.5.174)\u001b[0m     task = self.raylet_client.get_task()\n",
      "\u001b[2m\u001b[36m(pid=5927, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 291, in ray._raylet.RayletClient.get_task\n",
      "\u001b[2m\u001b[36m(pid=5927, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=5927, ip=10.21.5.174)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Raylet connection closed.\n",
      "\u001b[2m\u001b[36m(pid=5927, ip=10.21.5.174)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5927, ip=10.21.5.174)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=5927, ip=10.21.5.174)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5927, ip=10.21.5.174)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=5927, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 105, in <module>\n",
      "\u001b[2m\u001b[36m(pid=5927, ip=10.21.5.174)\u001b[0m     job_id=None)\n",
      "\u001b[2m\u001b[36m(pid=5927, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/utils.py\", line 67, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=5927, ip=10.21.5.174)\u001b[0m     worker.raylet_client.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=5927, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 327, in ray._raylet.RayletClient.push_error\n",
      "\u001b[2m\u001b[36m(pid=5927, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=5927, ip=10.21.5.174)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.\n",
      "\u001b[2m\u001b[36m(pid=11583, ip=10.21.5.173)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=11583, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 98, in <module>\n",
      "\u001b[2m\u001b[36m(pid=11583, ip=10.21.5.173)\u001b[0m     ray.worker.global_worker.main_loop()\n",
      "\u001b[2m\u001b[36m(pid=11583, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1118, in main_loop\n",
      "\u001b[2m\u001b[36m(pid=11583, ip=10.21.5.173)\u001b[0m E0102 20:54:10.131649 11662 raylet_client.cc:339] IOError: [RayletClient] Connection closed unexpectedly. [RayletClient] Failed to push profile events.\n",
      "\u001b[2m\u001b[36m(pid=11583, ip=10.21.5.173)\u001b[0m     task = self._get_next_task_from_raylet()\n",
      "\u001b[2m\u001b[36m(pid=11583, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1101, in _get_next_task_from_raylet\n",
      "\u001b[2m\u001b[36m(pid=11583, ip=10.21.5.173)\u001b[0m     task = self.raylet_client.get_task()\n",
      "\u001b[2m\u001b[36m(pid=11583, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 291, in ray._raylet.RayletClient.get_task\n",
      "\u001b[2m\u001b[36m(pid=11583, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=11583, ip=10.21.5.173)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Raylet connection closed.\n",
      "\u001b[2m\u001b[36m(pid=11583, ip=10.21.5.173)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11583, ip=10.21.5.173)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=11583, ip=10.21.5.173)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11583, ip=10.21.5.173)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=11583, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 105, in <module>\n",
      "\u001b[2m\u001b[36m(pid=11583, ip=10.21.5.173)\u001b[0m     job_id=None)\n",
      "\u001b[2m\u001b[36m(pid=11583, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/utils.py\", line 67, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=11583, ip=10.21.5.173)\u001b[0m     worker.raylet_client.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=11583, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 327, in ray._raylet.RayletClient.push_error\n",
      "\u001b[2m\u001b[36m(pid=11583, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=11583, ip=10.21.5.173)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.\n",
      "\u001b[2m\u001b[36m(pid=5930, ip=10.21.5.174)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=5930, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 98, in <module>\n",
      "\u001b[2m\u001b[36m(pid=5930, ip=10.21.5.174)\u001b[0m     ray.worker.global_worker.main_loop()\n",
      "\u001b[2m\u001b[36m(pid=5930, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1118, in main_loop\n",
      "\u001b[2m\u001b[36m(pid=5930, ip=10.21.5.174)\u001b[0m     task = self._get_next_task_from_raylet()\n",
      "\u001b[2m\u001b[36m(pid=5930, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1101, in _get_next_task_from_raylet\n",
      "\u001b[2m\u001b[36m(pid=5930, ip=10.21.5.174)\u001b[0m     task = self.raylet_client.get_task()\n",
      "\u001b[2m\u001b[36m(pid=5930, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 291, in ray._raylet.RayletClient.get_task\n",
      "\u001b[2m\u001b[36m(pid=5930, ip=10.21.5.174)\u001b[0m E0102 20:54:09.835765  6013 raylet_client.cc:339] IOError: [RayletClient] Connection closed unexpectedly. [RayletClient] Failed to push profile events.\n",
      "\u001b[2m\u001b[36m(pid=5930, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=5930, ip=10.21.5.174)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Raylet connection closed.\n",
      "\u001b[2m\u001b[36m(pid=5930, ip=10.21.5.174)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5930, ip=10.21.5.174)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=5930, ip=10.21.5.174)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5930, ip=10.21.5.174)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=5930, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 105, in <module>\n",
      "\u001b[2m\u001b[36m(pid=5930, ip=10.21.5.174)\u001b[0m     job_id=None)\n",
      "\u001b[2m\u001b[36m(pid=5930, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/utils.py\", line 67, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=5930, ip=10.21.5.174)\u001b[0m     worker.raylet_client.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=5930, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 327, in ray._raylet.RayletClient.push_error\n",
      "\u001b[2m\u001b[36m(pid=5930, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=5930, ip=10.21.5.174)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.\n",
      "\u001b[2m\u001b[36m(pid=11584, ip=10.21.5.173)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=11584, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 98, in <module>\n",
      "\u001b[2m\u001b[36m(pid=11584, ip=10.21.5.173)\u001b[0m     ray.worker.global_worker.main_loop()\n",
      "\u001b[2m\u001b[36m(pid=11584, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1118, in main_loop\n",
      "\u001b[2m\u001b[36m(pid=11584, ip=10.21.5.173)\u001b[0m     task = self._get_next_task_from_raylet()\n",
      "\u001b[2m\u001b[36m(pid=11584, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1101, in _get_next_task_from_raylet\n",
      "\u001b[2m\u001b[36m(pid=11584, ip=10.21.5.173)\u001b[0m     task = self.raylet_client.get_task()\n",
      "\u001b[2m\u001b[36m(pid=11584, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 291, in ray._raylet.RayletClient.get_task\n",
      "\u001b[2m\u001b[36m(pid=11584, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=11584, ip=10.21.5.173)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Raylet connection closed.\n",
      "\u001b[2m\u001b[36m(pid=11584, ip=10.21.5.173)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11584, ip=10.21.5.173)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=11584, ip=10.21.5.173)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11584, ip=10.21.5.173)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=11584, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 105, in <module>\n",
      "\u001b[2m\u001b[36m(pid=11584, ip=10.21.5.173)\u001b[0m     job_id=None)\n",
      "\u001b[2m\u001b[36m(pid=11584, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/utils.py\", line 67, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=11584, ip=10.21.5.173)\u001b[0m E0102 20:54:10.150701 11660 raylet_client.cc:339] IOError: [RayletClient] Connection closed unexpectedly. [RayletClient] Failed to push profile events.\n",
      "\u001b[2m\u001b[36m(pid=11584, ip=10.21.5.173)\u001b[0m     worker.raylet_client.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=11584, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 327, in ray._raylet.RayletClient.push_error\n",
      "\u001b[2m\u001b[36m(pid=11584, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=11584, ip=10.21.5.173)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 936, in _process_task\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m     outputs = function_executor(*arguments)\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m   File \"<ipython-input-4-869321c4bd9a>\", line 86, in worker_task\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 2343, in get\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m     values = worker.get_objects(object_ids)\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 579, in get_objects\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m     results = self.retrieve_and_deserialize(object_ids)\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 484, in retrieve_and_deserialize\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m     object_ids, self.current_task_id)\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 433, in ray._raylet.CoreWorker.get_objects\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 98, in <module>\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m     ray.worker.global_worker.main_loop()\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1119, in main_loop\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m     self._wait_for_and_process_task(task)\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1068, in _wait_for_and_process_task\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m     self._process_task(task, execution_info)\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 965, in _process_task\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m     function_descriptor, return_object_ids, e, traceback_str)\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1014, in _handle_process_task_failure\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m     job_id=self.current_job_id)\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/utils.py\", line 67, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m     worker.raylet_client.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 327, in ray._raylet.RayletClient.push_error\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 105, in <module>\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m     job_id=None)\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/utils.py\", line 67, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m     worker.raylet_client.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 327, in ray._raylet.RayletClient.push_error\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=5929, ip=10.21.5.174)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.\n",
      "\u001b[2m\u001b[36m(pid=5924, ip=10.21.5.174)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=5924, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 98, in <module>\n",
      "\u001b[2m\u001b[36m(pid=5924, ip=10.21.5.174)\u001b[0m     ray.worker.global_worker.main_loop()\n",
      "\u001b[2m\u001b[36m(pid=5924, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1118, in main_loop\n",
      "\u001b[2m\u001b[36m(pid=5924, ip=10.21.5.174)\u001b[0m     task = self._get_next_task_from_raylet()\n",
      "\u001b[2m\u001b[36m(pid=5924, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1101, in _get_next_task_from_raylet\n",
      "\u001b[2m\u001b[36m(pid=5924, ip=10.21.5.174)\u001b[0m     task = self.raylet_client.get_task()\n",
      "\u001b[2m\u001b[36m(pid=5924, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 291, in ray._raylet.RayletClient.get_task\n",
      "\u001b[2m\u001b[36m(pid=5924, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=5924, ip=10.21.5.174)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Raylet connection closed.\n",
      "\u001b[2m\u001b[36m(pid=5924, ip=10.21.5.174)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5924, ip=10.21.5.174)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=5924, ip=10.21.5.174)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5924, ip=10.21.5.174)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=5924, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 105, in <module>\n",
      "\u001b[2m\u001b[36m(pid=5924, ip=10.21.5.174)\u001b[0m     job_id=None)\n",
      "\u001b[2m\u001b[36m(pid=5924, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/utils.py\", line 67, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=5924, ip=10.21.5.174)\u001b[0m     worker.raylet_client.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=5924, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 327, in ray._raylet.RayletClient.push_error\n",
      "\u001b[2m\u001b[36m(pid=5924, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=5924, ip=10.21.5.174)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.\n",
      "\u001b[2m\u001b[36m(pid=11585, ip=10.21.5.173)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=11585, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 98, in <module>\n",
      "\u001b[2m\u001b[36m(pid=11585, ip=10.21.5.173)\u001b[0m     ray.worker.global_worker.main_loop()\n",
      "\u001b[2m\u001b[36m(pid=11585, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1118, in main_loop\n",
      "\u001b[2m\u001b[36m(pid=11585, ip=10.21.5.173)\u001b[0m     task = self._get_next_task_from_raylet()\n",
      "\u001b[2m\u001b[36m(pid=11585, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1101, in _get_next_task_from_raylet\n",
      "\u001b[2m\u001b[36m(pid=11585, ip=10.21.5.173)\u001b[0m     task = self.raylet_client.get_task()\n",
      "\u001b[2m\u001b[36m(pid=11585, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 291, in ray._raylet.RayletClient.get_task\n",
      "\u001b[2m\u001b[36m(pid=11585, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=11585, ip=10.21.5.173)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Raylet connection closed.\n",
      "\u001b[2m\u001b[36m(pid=11585, ip=10.21.5.173)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11585, ip=10.21.5.173)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=11585, ip=10.21.5.173)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11585, ip=10.21.5.173)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=11585, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 105, in <module>\n",
      "\u001b[2m\u001b[36m(pid=11585, ip=10.21.5.173)\u001b[0m     job_id=None)\n",
      "\u001b[2m\u001b[36m(pid=11585, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/utils.py\", line 67, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=11585, ip=10.21.5.173)\u001b[0m     worker.raylet_client.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=11585, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 327, in ray._raylet.RayletClient.push_error\n",
      "\u001b[2m\u001b[36m(pid=11585, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=11585, ip=10.21.5.173)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.\n",
      "\u001b[2m\u001b[36m(pid=5926, ip=10.21.5.174)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=5926, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 98, in <module>\n",
      "\u001b[2m\u001b[36m(pid=5926, ip=10.21.5.174)\u001b[0m     ray.worker.global_worker.main_loop()\n",
      "\u001b[2m\u001b[36m(pid=5926, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1118, in main_loop\n",
      "\u001b[2m\u001b[36m(pid=5926, ip=10.21.5.174)\u001b[0m     task = self._get_next_task_from_raylet()\n",
      "\u001b[2m\u001b[36m(pid=5926, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1101, in _get_next_task_from_raylet\n",
      "\u001b[2m\u001b[36m(pid=5926, ip=10.21.5.174)\u001b[0m     task = self.raylet_client.get_task()\n",
      "\u001b[2m\u001b[36m(pid=5926, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 291, in ray._raylet.RayletClient.get_task\n",
      "\u001b[2m\u001b[36m(pid=5926, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=5926, ip=10.21.5.174)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Raylet connection closed.\n",
      "\u001b[2m\u001b[36m(pid=5926, ip=10.21.5.174)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5926, ip=10.21.5.174)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=5926, ip=10.21.5.174)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5926, ip=10.21.5.174)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=5926, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 105, in <module>\n",
      "\u001b[2m\u001b[36m(pid=5926, ip=10.21.5.174)\u001b[0m     job_id=None)\n",
      "\u001b[2m\u001b[36m(pid=5926, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/utils.py\", line 67, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=5926, ip=10.21.5.174)\u001b[0m     worker.raylet_client.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=5926, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 327, in ray._raylet.RayletClient.push_error\n",
      "\u001b[2m\u001b[36m(pid=5926, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=5926, ip=10.21.5.174)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.\n",
      "\u001b[2m\u001b[36m(pid=11581, ip=10.21.5.173)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=11581, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 98, in <module>\n",
      "\u001b[2m\u001b[36m(pid=11581, ip=10.21.5.173)\u001b[0m     ray.worker.global_worker.main_loop()\n",
      "\u001b[2m\u001b[36m(pid=11581, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1118, in main_loop\n",
      "\u001b[2m\u001b[36m(pid=11581, ip=10.21.5.173)\u001b[0m     task = self._get_next_task_from_raylet()\n",
      "\u001b[2m\u001b[36m(pid=11581, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1101, in _get_next_task_from_raylet\n",
      "\u001b[2m\u001b[36m(pid=11581, ip=10.21.5.173)\u001b[0m     task = self.raylet_client.get_task()\n",
      "\u001b[2m\u001b[36m(pid=11581, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 291, in ray._raylet.RayletClient.get_task\n",
      "\u001b[2m\u001b[36m(pid=11581, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=11581, ip=10.21.5.173)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Raylet connection closed.\n",
      "\u001b[2m\u001b[36m(pid=11581, ip=10.21.5.173)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11581, ip=10.21.5.173)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=11581, ip=10.21.5.173)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11581, ip=10.21.5.173)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=11581, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 105, in <module>\n",
      "\u001b[2m\u001b[36m(pid=11581, ip=10.21.5.173)\u001b[0m     job_id=None)\n",
      "\u001b[2m\u001b[36m(pid=11581, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/utils.py\", line 67, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=11581, ip=10.21.5.173)\u001b[0m     worker.raylet_client.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=11581, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 327, in ray._raylet.RayletClient.push_error\n",
      "\u001b[2m\u001b[36m(pid=11581, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=11581, ip=10.21.5.173)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.\n",
      "\u001b[2m\u001b[36m(pid=5923, ip=10.21.5.174)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=5923, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 98, in <module>\n",
      "\u001b[2m\u001b[36m(pid=5923, ip=10.21.5.174)\u001b[0m     ray.worker.global_worker.main_loop()\n",
      "\u001b[2m\u001b[36m(pid=5923, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1118, in main_loop\n",
      "\u001b[2m\u001b[36m(pid=5923, ip=10.21.5.174)\u001b[0m     task = self._get_next_task_from_raylet()\n",
      "\u001b[2m\u001b[36m(pid=5923, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1101, in _get_next_task_from_raylet\n",
      "\u001b[2m\u001b[36m(pid=5923, ip=10.21.5.174)\u001b[0m     task = self.raylet_client.get_task()\n",
      "\u001b[2m\u001b[36m(pid=5923, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 291, in ray._raylet.RayletClient.get_task\n",
      "\u001b[2m\u001b[36m(pid=5923, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=5923, ip=10.21.5.174)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Raylet connection closed.\n",
      "\u001b[2m\u001b[36m(pid=5923, ip=10.21.5.174)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5923, ip=10.21.5.174)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=5923, ip=10.21.5.174)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5923, ip=10.21.5.174)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=5923, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 105, in <module>\n",
      "\u001b[2m\u001b[36m(pid=5923, ip=10.21.5.174)\u001b[0m E0102 20:54:09.849567  6005 raylet_client.cc:339] IOError: [RayletClient] Connection closed unexpectedly. [RayletClient] Failed to push profile events.\n",
      "\u001b[2m\u001b[36m(pid=5923, ip=10.21.5.174)\u001b[0m     job_id=None)\n",
      "\u001b[2m\u001b[36m(pid=5923, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/utils.py\", line 67, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=5923, ip=10.21.5.174)\u001b[0m     worker.raylet_client.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=5923, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 327, in ray._raylet.RayletClient.push_error\n",
      "\u001b[2m\u001b[36m(pid=5923, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=5923, ip=10.21.5.174)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.\n",
      "\u001b[2m\u001b[36m(pid=5925, ip=10.21.5.174)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=5925, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 98, in <module>\n",
      "\u001b[2m\u001b[36m(pid=5925, ip=10.21.5.174)\u001b[0m     ray.worker.global_worker.main_loop()\n",
      "\u001b[2m\u001b[36m(pid=5925, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1118, in main_loop\n",
      "\u001b[2m\u001b[36m(pid=5925, ip=10.21.5.174)\u001b[0m     task = self._get_next_task_from_raylet()\n",
      "\u001b[2m\u001b[36m(pid=5925, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1101, in _get_next_task_from_raylet\n",
      "\u001b[2m\u001b[36m(pid=5925, ip=10.21.5.174)\u001b[0m     task = self.raylet_client.get_task()\n",
      "\u001b[2m\u001b[36m(pid=5925, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 291, in ray._raylet.RayletClient.get_task\n",
      "\u001b[2m\u001b[36m(pid=5925, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=5925, ip=10.21.5.174)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Raylet connection closed.\n",
      "\u001b[2m\u001b[36m(pid=5925, ip=10.21.5.174)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5925, ip=10.21.5.174)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=5925, ip=10.21.5.174)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5925, ip=10.21.5.174)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=5925, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 105, in <module>\n",
      "\u001b[2m\u001b[36m(pid=5925, ip=10.21.5.174)\u001b[0m     job_id=None)\n",
      "\u001b[2m\u001b[36m(pid=5925, ip=10.21.5.174)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/utils.py\", line 67, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=5925, ip=10.21.5.174)\u001b[0m     worker.raylet_client.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=5925, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 327, in ray._raylet.RayletClient.push_error\n",
      "\u001b[2m\u001b[36m(pid=5925, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=5925, ip=10.21.5.174)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.\n",
      "\u001b[2m\u001b[36m(pid=11582, ip=10.21.5.173)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=11582, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 98, in <module>\n",
      "\u001b[2m\u001b[36m(pid=11582, ip=10.21.5.173)\u001b[0m     ray.worker.global_worker.main_loop()\n",
      "\u001b[2m\u001b[36m(pid=11582, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1118, in main_loop\n",
      "\u001b[2m\u001b[36m(pid=11582, ip=10.21.5.173)\u001b[0m E0102 20:54:10.131062 11667 raylet_client.cc:339] IOError: [RayletClient] Connection closed unexpectedly. [RayletClient] Failed to push profile events.\n",
      "\u001b[2m\u001b[36m(pid=11582, ip=10.21.5.173)\u001b[0m     task = self._get_next_task_from_raylet()\n",
      "\u001b[2m\u001b[36m(pid=11582, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1101, in _get_next_task_from_raylet\n",
      "\u001b[2m\u001b[36m(pid=11582, ip=10.21.5.173)\u001b[0m     task = self.raylet_client.get_task()\n",
      "\u001b[2m\u001b[36m(pid=11582, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 291, in ray._raylet.RayletClient.get_task\n",
      "\u001b[2m\u001b[36m(pid=11582, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=11582, ip=10.21.5.173)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Raylet connection closed.\n",
      "\u001b[2m\u001b[36m(pid=11582, ip=10.21.5.173)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11582, ip=10.21.5.173)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=11582, ip=10.21.5.173)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11582, ip=10.21.5.173)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=11582, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 105, in <module>\n",
      "\u001b[2m\u001b[36m(pid=11582, ip=10.21.5.173)\u001b[0m     job_id=None)\n",
      "\u001b[2m\u001b[36m(pid=11582, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/utils.py\", line 67, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=11582, ip=10.21.5.173)\u001b[0m     worker.raylet_client.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=11582, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 327, in ray._raylet.RayletClient.push_error\n",
      "\u001b[2m\u001b[36m(pid=11582, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=11582, ip=10.21.5.173)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 936, in _process_task\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m     outputs = function_executor(*arguments)\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m   File \"<ipython-input-4-869321c4bd9a>\", line 86, in worker_task\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m E0102 20:54:10.630429 11669 raylet_client.cc:339] IOError: [RayletClient] Connection closed unexpectedly. [RayletClient] Failed to push profile events.\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 2343, in get\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m     values = worker.get_objects(object_ids)\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 579, in get_objects\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m     results = self.retrieve_and_deserialize(object_ids)\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 484, in retrieve_and_deserialize\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m     object_ids, self.current_task_id)\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 433, in ray._raylet.CoreWorker.get_objects\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 98, in <module>\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m     ray.worker.global_worker.main_loop()\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1119, in main_loop\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m     self._wait_for_and_process_task(task)\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1068, in _wait_for_and_process_task\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m     self._process_task(task, execution_info)\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 965, in _process_task\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m     function_descriptor, return_object_ids, e, traceback_str)\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1014, in _handle_process_task_failure\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m     job_id=self.current_job_id)\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/utils.py\", line 67, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m     worker.raylet_client.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 327, in ray._raylet.RayletClient.push_error\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m During handling of the above exception, another exception occurred:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-02 20:54:44,189\tWARNING worker.py:1779 -- The node with client ID 78f664ff7444c56bbeceb0abcef000abfd678024 has been marked dead because the monitor has missed too many heartbeats from it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 105, in <module>\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m     job_id=None)\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/utils.py\", line 67, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m     worker.raylet_client.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 327, in ray._raylet.RayletClient.push_error\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=11586, ip=10.21.5.173)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.\n",
      "\u001b[2m\u001b[36m(pid=5928, ip=10.21.5.174)\u001b[0m     worker.raylet_client.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=5928, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 327, in ray._raylet.RayletClient.push_error\n",
      "\u001b[2m\u001b[36m(pid=5928, ip=10.21.5.174)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=5928, ip=10.21.5.174)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.\n",
      "\u001b[2m\u001b[36m(pid=11580, ip=10.21.5.173)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=11580, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 98, in <module>\n",
      "\u001b[2m\u001b[36m(pid=11580, ip=10.21.5.173)\u001b[0m     ray.worker.global_worker.main_loop()\n",
      "\u001b[2m\u001b[36m(pid=11580, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1118, in main_loop\n",
      "\u001b[2m\u001b[36m(pid=11580, ip=10.21.5.173)\u001b[0m     task = self._get_next_task_from_raylet()\n",
      "\u001b[2m\u001b[36m(pid=11580, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1101, in _get_next_task_from_raylet\n",
      "\u001b[2m\u001b[36m(pid=11580, ip=10.21.5.173)\u001b[0m     task = self.raylet_client.get_task()\n",
      "\u001b[2m\u001b[36m(pid=11580, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 291, in ray._raylet.RayletClient.get_task\n",
      "\u001b[2m\u001b[36m(pid=11580, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=11580, ip=10.21.5.173)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Raylet connection closed.\n",
      "\u001b[2m\u001b[36m(pid=11580, ip=10.21.5.173)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11580, ip=10.21.5.173)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=11580, ip=10.21.5.173)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11580, ip=10.21.5.173)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=11580, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 105, in <module>\n",
      "\u001b[2m\u001b[36m(pid=11580, ip=10.21.5.173)\u001b[0m     job_id=None)\n",
      "\u001b[2m\u001b[36m(pid=11580, ip=10.21.5.173)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/utils.py\", line 67, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=11580, ip=10.21.5.173)\u001b[0m     worker.raylet_client.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=11580, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 327, in ray._raylet.RayletClient.push_error\n",
      "\u001b[2m\u001b[36m(pid=11580, ip=10.21.5.173)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=11580, ip=10.21.5.173)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m F0102 20:54:40.355222  8853 node_manager.cc:481]  Check failed: client_id != gcs_client_->client_table().GetLocalClientId() Exiting because this node manager has mistakenly been marked dead by the monitor.\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m *** Check failure stack trace: ***\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @           0x6f8d1a  google::LogMessage::Fail()\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @           0x6fa103  google::LogMessage::SendToLog()\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @           0x6f8a42  google::LogMessage::Flush()\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @           0x6f8c31  google::LogMessage::~LogMessage()\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @           0x52b0d2  ray::RayLog::~RayLog()\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @           0x466752  ray::raylet::NodeManager::ClientRemoved()\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @           0x4b790e  ray::gcs::ClientTable::HandleNotification()\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @           0x4d328b  _ZNSt17_Function_handlerIFvPN3ray3gcs14RedisGcsClientERKNS0_8ClientIDERKSt6vectorINS0_3rpc11GcsNodeInfoESaIS9_EEEZZNS1_11ClientTable7ConnectERKS9_ENKUlS3_RKNS0_8UniqueIDESH_E_clES3_SK_SH_EUlS3_SK_SD_E_E9_M_invokeERKSt9_Any_dataS3_S6_SD_\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @           0x4d2876  _ZNSt17_Function_handlerIFvPN3ray3gcs14RedisGcsClientERKNS0_8ClientIDENS0_3rpc13GcsChangeModeERKSt6vectorINS7_11GcsNodeInfoESaISA_EEEZNS1_3LogIS4_SA_E9SubscribeERKNS0_5JobIDES6_RKSt8functionIFvS3_S6_SE_EERKSL_IFvS3_EEEUlS3_S6_S8_SE_E_E9_M_invokeERKSt9_Any_dataS3_S6_S8_SE_\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @           0x4b5893  _ZZN3ray3gcs3LogINS_8ClientIDENS_3rpc11GcsNodeInfoEE9SubscribeERKNS_5JobIDERKS2_RKSt8functionIFvPNS0_14RedisGcsClientESA_NS3_13GcsChangeModeERKSt6vectorIS4_SaIS4_EEEERKSB_IFvSD_EEENKUlRKNS0_13CallbackReplyEE_clESU_\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @           0x4dad09  ray::gcs::GlobalRedisCallback()\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @           0x4dfa1b  redisProcessCallbacks\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @           0x4de776  RedisAsioClient::handle_read()\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @           0x4dd9a8  boost::asio::detail::reactive_null_buffers_op<>::do_complete()\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @           0x425bcd  boost::asio::detail::scheduler::run()\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @           0x40fb1d  main\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f0a650aab97  __libc_start_main\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @           0x4207e1  (unknown)\n",
      "\u001b[2m\u001b[36m(pid=8875)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 98, in <module>\n",
      "\u001b[2m\u001b[36m(pid=8871)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=8871)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 98, in <module>\n",
      "\u001b[2m\u001b[36m(pid=8871)\u001b[0m     ray.worker.global_worker.main_loop()\n",
      "\u001b[2m\u001b[36m(pid=8871)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1118, in main_loop\n",
      "\u001b[2m\u001b[36m(pid=8869)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=8869)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 98, in <module>\n",
      "\u001b[2m\u001b[36m(pid=8869)\u001b[0m     ray.worker.global_worker.main_loop()\n",
      "\u001b[2m\u001b[36m(pid=8869)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1118, in main_loop\n",
      "\u001b[2m\u001b[36m(pid=8870)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=8870)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 98, in <module>\n",
      "\u001b[2m\u001b[36m(pid=8870)\u001b[0m     ray.worker.global_worker.main_loop()\n",
      "\u001b[2m\u001b[36m(pid=8870)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1118, in main_loop\n",
      "\u001b[2m\u001b[36m(pid=8872)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=8872)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 98, in <module>\n",
      "\u001b[2m\u001b[36m(pid=8872)\u001b[0m     ray.worker.global_worker.main_loop()\n",
      "\u001b[2m\u001b[36m(pid=8872)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1118, in main_loop\n",
      "\u001b[2m\u001b[36m(pid=8875)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 98, in <module>\n",
      "\u001b[2m\u001b[36m(pid=8875)\u001b[0m     ray.worker.global_worker.main_loop()\n",
      "\u001b[2m\u001b[36m(pid=8875)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1118, in main_loop\n",
      "\u001b[2m\u001b[36m(pid=8870)\u001b[0m     task = self._get_next_task_from_raylet()\n",
      "\u001b[2m\u001b[36m(pid=8870)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1101, in _get_next_task_from_raylet\n",
      "\u001b[2m\u001b[36m(pid=8870)\u001b[0m     task = self.raylet_client.get_task()\n",
      "\u001b[2m\u001b[36m(pid=8870)\u001b[0m   File \"python/ray/_raylet.pyx\", line 291, in ray._raylet.RayletClient.get_task\n",
      "\u001b[2m\u001b[36m(pid=8870)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=8870)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Raylet connection closed.\n",
      "\u001b[2m\u001b[36m(pid=8870)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8870)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=8870)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8870)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=8870)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 105, in <module>\n",
      "\u001b[2m\u001b[36m(pid=8870)\u001b[0m     job_id=None)\n",
      "\u001b[2m\u001b[36m(pid=8870)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/utils.py\", line 67, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=8870)\u001b[0m     worker.raylet_client.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=8870)\u001b[0m   File \"python/ray/_raylet.pyx\", line 327, in ray._raylet.RayletClient.push_error\n",
      "\u001b[2m\u001b[36m(pid=8870)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=8870)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.\n",
      "\u001b[2m\u001b[36m(pid=8872)\u001b[0m     task = self._get_next_task_from_raylet()\n",
      "\u001b[2m\u001b[36m(pid=8872)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1101, in _get_next_task_from_raylet\n",
      "\u001b[2m\u001b[36m(pid=8872)\u001b[0m     task = self.raylet_client.get_task()\n",
      "\u001b[2m\u001b[36m(pid=8872)\u001b[0m   File \"python/ray/_raylet.pyx\", line 291, in ray._raylet.RayletClient.get_task\n",
      "\u001b[2m\u001b[36m(pid=8872)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=8872)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Raylet connection closed.\n",
      "\u001b[2m\u001b[36m(pid=8872)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8872)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=8872)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8872)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=8872)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 105, in <module>\n",
      "\u001b[2m\u001b[36m(pid=8872)\u001b[0m     job_id=None)\n",
      "\u001b[2m\u001b[36m(pid=8872)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/utils.py\", line 67, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=8872)\u001b[0m     worker.raylet_client.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=8872)\u001b[0m   File \"python/ray/_raylet.pyx\", line 327, in ray._raylet.RayletClient.push_error\n",
      "\u001b[2m\u001b[36m(pid=8872)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=8872)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.\n",
      "\u001b[2m\u001b[36m(pid=8875)\u001b[0m     task = self._get_next_task_from_raylet()\n",
      "\u001b[2m\u001b[36m(pid=8875)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1101, in _get_next_task_from_raylet\n",
      "\u001b[2m\u001b[36m(pid=8875)\u001b[0m     task = self.raylet_client.get_task()\n",
      "\u001b[2m\u001b[36m(pid=8875)\u001b[0m   File \"python/ray/_raylet.pyx\", line 291, in ray._raylet.RayletClient.get_task\n",
      "\u001b[2m\u001b[36m(pid=8875)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=8875)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Raylet connection closed.\n",
      "\u001b[2m\u001b[36m(pid=8875)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8875)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=8875)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8875)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=8875)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 105, in <module>\n",
      "\u001b[2m\u001b[36m(pid=8875)\u001b[0m     job_id=None)\n",
      "\u001b[2m\u001b[36m(pid=8875)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/utils.py\", line 67, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=8875)\u001b[0m     worker.raylet_client.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=8875)\u001b[0m   File \"python/ray/_raylet.pyx\", line 327, in ray._raylet.RayletClient.push_error\n",
      "\u001b[2m\u001b[36m(pid=8875)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=8875)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m     ray.worker.global_worker.main_loop()\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1118, in main_loop\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m     task = self._get_next_task_from_raylet()\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1101, in _get_next_task_from_raylet\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m     task = self.raylet_client.get_task()\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m   File \"python/ray/_raylet.pyx\", line 291, in ray._raylet.RayletClient.get_task\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Raylet connection closed.\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 105, in <module>\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m     job_id=None)\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/utils.py\", line 67, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m     worker.raylet_client.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m   File \"python/ray/_raylet.pyx\", line 327, in ray._raylet.RayletClient.push_error\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=8873)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.\n",
      "\u001b[2m\u001b[36m(pid=8871)\u001b[0m     task = self._get_next_task_from_raylet()\n",
      "\u001b[2m\u001b[36m(pid=8871)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1101, in _get_next_task_from_raylet\n",
      "\u001b[2m\u001b[36m(pid=8871)\u001b[0m     task = self.raylet_client.get_task()\n",
      "\u001b[2m\u001b[36m(pid=8871)\u001b[0m   File \"python/ray/_raylet.pyx\", line 291, in ray._raylet.RayletClient.get_task\n",
      "\u001b[2m\u001b[36m(pid=8871)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=8871)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Raylet connection closed.\n",
      "\u001b[2m\u001b[36m(pid=8871)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8871)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=8871)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8871)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=8871)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 105, in <module>\n",
      "\u001b[2m\u001b[36m(pid=8871)\u001b[0m     job_id=None)\n",
      "\u001b[2m\u001b[36m(pid=8871)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/utils.py\", line 67, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=8871)\u001b[0m     worker.raylet_client.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=8871)\u001b[0m   File \"python/ray/_raylet.pyx\", line 327, in ray._raylet.RayletClient.push_error\n",
      "\u001b[2m\u001b[36m(pid=8871)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=8871)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.\n",
      "\u001b[2m\u001b[36m(pid=8869)\u001b[0m     task = self._get_next_task_from_raylet()\n",
      "\u001b[2m\u001b[36m(pid=8869)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1101, in _get_next_task_from_raylet\n",
      "\u001b[2m\u001b[36m(pid=8869)\u001b[0m     task = self.raylet_client.get_task()\n",
      "\u001b[2m\u001b[36m(pid=8869)\u001b[0m   File \"python/ray/_raylet.pyx\", line 291, in ray._raylet.RayletClient.get_task\n",
      "\u001b[2m\u001b[36m(pid=8869)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=8869)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Raylet connection closed.\n",
      "\u001b[2m\u001b[36m(pid=8869)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8869)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=8869)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8869)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=8869)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 105, in <module>\n",
      "\u001b[2m\u001b[36m(pid=8869)\u001b[0m     job_id=None)\n",
      "\u001b[2m\u001b[36m(pid=8869)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/utils.py\", line 67, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=8869)\u001b[0m     worker.raylet_client.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=8869)\u001b[0m   File \"python/ray/_raylet.pyx\", line 327, in ray._raylet.RayletClient.push_error\n",
      "\u001b[2m\u001b[36m(pid=8869)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=8869)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 936, in _process_task\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m     outputs = function_executor(*arguments)\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m   File \"<ipython-input-4-869321c4bd9a>\", line 86, in worker_task\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/actor.py\", line 148, in remote\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m     return self._remote(args, kwargs)\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/actor.py\", line 169, in _remote\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m     return invocation(args, kwargs)\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/actor.py\", line 163, in invocation\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m     num_return_vals=num_return_vals)\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/actor.py\", line 607, in _actor_method_call\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m     job_id=self._ray_actor_job_id,\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 737, in submit_task\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m     self.raylet_client.submit_task(task)\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m   File \"python/ray/_raylet.pyx\", line 283, in ray._raylet.RayletClient.submit_task\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 98, in <module>\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m     ray.worker.global_worker.main_loop()\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1119, in main_loop\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m     self._wait_for_and_process_task(task)\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1068, in _wait_for_and_process_task\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m     self._process_task(task, execution_info)\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 965, in _process_task\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m     function_descriptor, return_object_ids, e, traceback_str)\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/worker.py\", line 1014, in _handle_process_task_failure\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m     job_id=self.current_job_id)\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/utils.py\", line 67, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m     worker.raylet_client.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m   File \"python/ray/_raylet.pyx\", line 327, in ray._raylet.RayletClient.push_error\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/workers/default_worker.py\", line 105, in <module>\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m     job_id=None)\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m   File \"/userhome/34/gyu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ray/utils.py\", line 67, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m     worker.raylet_client.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m   File \"python/ray/_raylet.pyx\", line 327, in ray._raylet.RayletClient.push_error\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m   File \"python/ray/_raylet.pyx\", line 98, in ray._raylet.check_status\n",
      "\u001b[2m\u001b[36m(pid=8868)\u001b[0m ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.\n"
     ]
    }
   ],
   "source": [
    "worker_tasks = [worker_task.remote(args,ps,idx,train_loaders[idx]) for idx in range(args.num_workers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ray.get(ps.get_stalness_table.remote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prune model \n",
    "%cd /userhome/34/gyu/git-repo/rethinking-network-pruning/cifar/network-slimming/\n",
    "!python resprune_modified.py --dataset cifar10 --depth 164 --percent 0.1 --model /userhome/34/gyu/logs_sr/3wk_p1/train4/checkpoint.pth.tar --save /userhome/34/gyu/logs_sr/3wk_p1/prune4/\n",
    "%cd /userhome/34/gyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ps.evaluate.remote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_test_loader = generate_test_loader(64)\n",
    "local_train_loader = generate_train_loader(64,{'num_workers': 1, 'pin_memory': True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test_writer = SummaryWriter()\n",
    "\n",
    "\n",
    "checkpoint = torch.load('/userhome/34/gyu/logs_sr/3wk_p1/train24/checkpoint.pth.tar')\n",
    "local_test_model = models.__dict__[\"resnet\"](dataset=\"cifar10\",depth=164,cfg=checkpoint['cfg'])\n",
    "local_test_model.load_state_dict(checkpoint['state_dict'])\n",
    "local_test_model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pruned model\n",
    "local_test_loader = generate_test_loader(64)\n",
    "local_train_loader = generate_train_loader(64,{'num_workers': 1, 'pin_memory': True})\n",
    "\n",
    "test_writer = SummaryWriter()\n",
    "\n",
    "\n",
    "checkpoint = torch.load(args.refine)\n",
    "local_test_model = models.__dict__['resnet'](dataset='cifar10', depth=args.depth, cfg=checkpoint['cfg'])\n",
    "\n",
    "checkpoint = torch.load('/userhome/34/gyu/tmp/prune_40/ssp_refine_1wk_runningbn/checkpoint.pth.tar')\n",
    "local_test_model.load_state_dict(checkpoint['state_dict'])\n",
    "local_test_model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(local_test_model, input_size=(3, 32, 32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "state = local_test_model.state_dict()\n",
    "for idx, p in enumerate(state):\n",
    "    data.append(state[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0][0][0][0][0] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=[]\n",
    "state2 = local_test_model.state_dict()\n",
    "for idx, p in enumerate(state2):\n",
    "    data2.append(state2[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2[0][0][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state1 = local_test_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state1['layer1.0.bn1.running_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state1['layer1.0.bn1.running_var']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state1['layer1.0.bn1.weight']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state1['layer1.0.bn1.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_test_model = ray.get(ps.get_model.remote())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim= ray.get(ps.get_optim.remote())\n",
    "# local_test_model = ray.get(ps.get_model.remote())\n",
    "# cfg = ray.get(ps.pull_cfg.remote())\n",
    "\n",
    "# torch.save({'epoch':3,\n",
    "#                 'global_step':8601,\n",
    "#                 'state_dict':local_test_model.state_dict(),\n",
    "#                 'optimizer':optim.state_dict(),\n",
    "#                 'cfg':cfg\n",
    "#             },'/userhome/34/gyu/logs_sr/3wk_p1/prune_2nd/checkpoint.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_dataloader):\n",
    "    local_test_model = model\n",
    "    local_test_loader = test_dataloader\n",
    "    # local_test_model.train()\n",
    "    local_test_model.eval()\n",
    "    # test dataset loader \n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    batch_count = 0.\n",
    "    for data, target in local_test_loader:\n",
    "        data,target = data.cuda(),target.cuda()\n",
    "        batch_count += 1\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = local_test_model(data)\n",
    "        test_loss += F.cross_entropy(output, target, size_average=False).data # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        batch_correct = pred.eq(target.data.view_as(pred)).sum()\n",
    "        correct += batch_correct\n",
    "#         if batch_count % 100  == 0:\n",
    "#             print(\"        with model.eval(), batch num: \",batch_count, \" with correct: \",int(batch_correct.data), \" / \",len(data))\n",
    "\n",
    "    test_loss /= len(local_test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.6f})\\n'.format(\n",
    "        test_loss, correct, len(local_test_loader.dataset),\n",
    "        correct / float(len(local_test_loader.dataset))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_test_model.eval()\n",
    "local_test_model.cuda()\n",
    "test_model(local_test_model,local_test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 经过一个epoch 的 test set\n",
    "state2 = local_test_model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state2['layer1.0.bn1.running_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state2['layer1.0.bn1.running_var']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state2['layer1.0.bn1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state2['layer1.0.bn1.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bn层的4个参数都没有变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset loader, but set model.eval(), acc=11% for one epoch \n",
    "# local_test_model.eval()\n",
    "local_test_model.train()\n",
    "# train dataset loader\n",
    "\n",
    "test_loss = 0.\n",
    "correct = 0.\n",
    "train_batch_count = 0.\n",
    "num_batch = 0\n",
    "for data, target in local_train_loader:\n",
    "#     if train_batch_count % 30 == 0:\n",
    "#         print(train_batch_count)\n",
    "#         local_test_model.eval()\n",
    "#         test_model(local_test_model,local_test_loader)\n",
    "        \n",
    "    if train_batch_count==150:\n",
    "        break\n",
    "        \n",
    "    num_batch += 1   \n",
    "    local_test_model.train()\n",
    "    data,target = data.cuda(),target.cuda()\n",
    "    train_batch_count += 1\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    output = local_test_model(data)\n",
    "    test_loss += F.cross_entropy(output, target, size_average=False).data # sum up batch loss\n",
    "    pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "    batch_correct = pred.eq(target.data.view_as(pred)).sum()\n",
    "    correct += batch_correct\n",
    "    \n",
    "    for name,param in local_test_model.named_parameters():\n",
    "        test_writer.add_histogram(name, param, num_batch)\n",
    "\n",
    "    \n",
    "#     if train_batch_count % 100  == 0:\n",
    "#         print(\"With model.train(), batch num: \",train_batch_count, \" , with correct: \",int(batch_correct.data), \" / \", len(data))\n",
    "\n",
    "test_loss /= len(local_train_loader.dataset)\n",
    "print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.6f})\\n'.format(\n",
    "    test_loss, correct, len(local_train_loader.dataset),\n",
    "    correct / float(len(local_train_loader.dataset))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state3 = local_test_model.state_dict()\n",
    "print(state3['layer1.0.bn1.running_mean'])\n",
    "print(state3['layer1.0.bn1.running_var'])\n",
    "print(state3['layer1.0.bn1.weight'])\n",
    "print(state3['layer1.0.bn1.bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_name = []\n",
    "bn_li = []\n",
    "for m in local_test_model.modules():\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        bn_name.append(m.name)\n",
    "        bn_li.append(m.weight.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_model = ray.get(ps.get_model.remote())\n",
    "# state = {\n",
    "#     'epoch':3,\n",
    "#     'global_step':6795,\n",
    "#     'state_dict':tmp_model.state_dict()\n",
    "# }\n",
    "# torch.save(state,'/userhome/34/gyu/logs_sr/3wk_p1/prune_1st/refine1/checkpoint_tmp.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_test_model2\n",
    "test_model(local_test_model2,local_test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_params(model):\n",
    "    bns = {}\n",
    "    non_bns = {}\n",
    "    param_count = 0.\n",
    "    bn_param_count = 0.\n",
    "    for name,param in model.named_parameters():\n",
    "        param_count += len(param)\n",
    "        if 'bn' in name:\n",
    "            bns[name] = param\n",
    "            bn_param_count += len(param)\n",
    "        else:\n",
    "            non_bns[name] = param\n",
    "    print(\"bn params occupies: \",bn_param_count/param_count)\n",
    "    return bns,non_bns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_get_params(local_test_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_bns2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = ray.get(ps.pull_weights.remote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_bn={}\n",
    "for ele in state_dict:\n",
    "    if 'bn' not in ele:\n",
    "        non_bn[ele]=state_dict[ele]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "init_wei = ray.get(ps.pull_weights.remote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_wei[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _to_remove_resnet.models as tmpmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res56 = tmpmodels.__dict__[\"resnet\"](dataset=\"cifar10\",depth=56)\n",
    "res56.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(res56, input_size=(3, 32, 32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "wide_50 = torch.hub.load('pytorch/vision:v0.4.2', 'wide_resnet50_2', pretrained=True)\n",
    "wide_50.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(wide_50, input_size=(3, 32, 32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnext50 = torch.hub.load('pytorch/vision:v0.4.2', 'resnext50_32x4d', pretrained=True)\n",
    "resnext50.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(resnext50, input_size=(3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
