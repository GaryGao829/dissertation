{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import ray\n",
    "import resnet.models as models\n",
    "import random,time\n",
    "from time import sleep\n",
    "import copy \n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init(address=\"10.21.5.172:11572\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "@ray.remote\n",
    "def f():\n",
    "    time.sleep(0.01)\n",
    "    return ray.services.get_node_ip_address()\n",
    "\n",
    "# Get a list of the IP addresses of the nodes that have joined the cluster.\n",
    "set(ray.get([f.remote() for _ in range(1000)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "@ray.remote(num_gpus=1,max_calls=1)\n",
    "def use_gpu():\n",
    "    print(\"ray.get_gpu_ids(): {}\".format(ray.get_gpu_ids()))\n",
    "    print(\"CUDA_VISIBLE_DEVICES: {}\".format(os.environ[\"CUDA_VISIBLE_DEVICES\"]))\n",
    "    \n",
    "[use_gpu.remote() for _ in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = \"resnet\"\n",
    "depth = 56\n",
    "cuda = torch.cuda.is_available()\n",
    "print(\"cuda is ready: \",cuda)\n",
    "# cuda = False\n",
    "seed = 1\n",
    "save = \"./logs\"\n",
    "dataset = \"cifar10\"\n",
    "batch_size = 64\n",
    "test_batch_size = 100\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "# ckpt_ssp_resnet = \"./ckpt_ssp_resnet/checkpoint.pth.tar\"\n",
    "ckpt_ssp_resnet = \"./ckpt_ssp_resnet\"\n",
    "lr = 0.1\n",
    "momentum=0.9\n",
    "weight_decay=1e-4 \n",
    "log_interval=100\n",
    "start_epoch = 0\n",
    "epochs=160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save):\n",
    "    os.makedirs(save)\n",
    "if not os.path.exists(ckpt_ssp_resnet):\n",
    "    os.makedirs(ckpt_ssp_resnet)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader1 = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('./data.cifar10', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.Pad(4),\n",
    "                       transforms.RandomCrop(32),\n",
    "                       transforms.RandomHorizontalFlip(),\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "train_loader2 = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('./data.cifar10', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.Pad(4),\n",
    "                       transforms.RandomCrop(32),\n",
    "                       transforms.RandomHorizontalFlip(),\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('./data.cifar10', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "                   ])),\n",
    "    batch_size=test_batch_size, shuffle=True, **{})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = [train_loader1,train_loader2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ray.remote(num_gpus=1)\n",
    "@ray.remote\n",
    "class ParameterServer():\n",
    "    def __init__(self,lr,num_workers,stalness_limit,test_loader,resume_from_ckpt):\n",
    "        self.lr = lr\n",
    "        self.model = models.__dict__[arch](dataset=dataset,depth=depth)\n",
    "        self.stalness_table = [0] * num_workers\n",
    "        self.stalness_limit = stalness_limit \n",
    "        self.global_step = 0\n",
    "        self.eva_model = models.__dict__[arch](dataset=dataset,depth=depth)\n",
    "#         self.eva_model.eval()\n",
    "        self.optimizer = optim.SGD(self.model.parameters(),\n",
    "                          lr=lr,\n",
    "                          momentum=momentum,\n",
    "                          weight_decay=weight_decay)\n",
    "        self.test_loader = test_loader\n",
    "        self.model.cpu()\n",
    "        self.eva_model.cpu()\n",
    "#         if cuda:\n",
    "#             self.model.cuda()\n",
    "#             self.eva_model.cuda()\n",
    "        # tensorboard logger\n",
    "        self.ps_writer = SummaryWriter()\n",
    "        \n",
    "        if resume_from_ckpt:\n",
    "            self.model.load_state_dict(torch.load(resume_from_ckpt))\n",
    "\n",
    "\n",
    "    def apply_gradients(self, gradients, wk_idx):\n",
    "        print(\"applying gradients from the \",wk_idx, \" worker\")\n",
    "        for idx, p in enumerate(self.model.parameters()):\n",
    "            p.data -= self.lr * gradients[idx]\n",
    "        self.stalness_table[wk_idx] += 1\n",
    "        self.global_step += 1\n",
    "        print(\"finished applying gradients from the \",wk_idx, \" worker\")\n",
    "        if self.global_step % 10 == 0:\n",
    "            print(\"global_step: \",self.global_step,\" and prepare evaluate\")\n",
    "            self.evaluate()\n",
    "        \n",
    "    def pull_weights(self):\n",
    "        return self.model.state_dict()\n",
    "    \n",
    "    def check_stalness(self,wk_idx):\n",
    "        min_iter = min(self.stalness_table)\n",
    "        return self.stalness_table[wk_idx] - min_iter < self.stalness_limit\n",
    "        \n",
    "    def get_stalness(self):\n",
    "        return min(self.stalness_table)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        print(\"going to evaluate\")\n",
    "        test_loss = 0.\n",
    "        correct = 0.\n",
    "        print(\"pulled weights\")\n",
    "        self.eva_model.load_state_dict(copy.deepcopy(self.model.state_dict()))\n",
    "        print(\"loaded weights\")\n",
    "        print(\"length of the test_loader dataset is : \",len(test_loader.dataset))\n",
    "        self.eva_model.eval()\n",
    "        batch = iter(test_loader)\n",
    "        data,target = next(batch)\n",
    "        data,target = Variable(data,volatile=True),Variable(target)\n",
    "        output = self.eva_model(data)\n",
    "        test_loss = F.cross_entropy(output,target,size_average=True)\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        print(\"evaling, get pred and going to cal correct\")\n",
    "#         correct = pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        correct = pred.eq(target.data.view_as(pred)).sum()\n",
    "        print(\"evaling, got correct\")\n",
    "        #log tensorboard\n",
    "        self.ps_writer.add_scalar('Accuracy/eval', (100.0 * correct) / len(data), self.global_step)\n",
    "        self.ps_writer.add_scalar('Loss/eval',test_loss , self.global_step)\n",
    "        \n",
    "        \n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
    "            test_loss, \n",
    "            correct, \n",
    "            len(data),\n",
    "            100. * correct / len(data)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_gpus=1)\n",
    "# @ray.remote\n",
    "def worker_task(ps,worker_index,stale_limit, train_loader,lr,momentum,weight_decay,batch_size=50):\n",
    "    # Initialize the model.\n",
    "    model = models.__dict__[arch](dataset=dataset,depth=depth)\n",
    "    local_step = 0\n",
    "    optimizer = optim.SGD(model.parameters(),\n",
    "                          lr=lr,\n",
    "                          momentum=momentum,\n",
    "                          weight_decay=weight_decay)\n",
    "    if cuda:\n",
    "        starttime = datetime.datetime.now()\n",
    "        model.cuda()\n",
    "        endtime = datetime.datetime.now()\n",
    "        time_cost = (endtime - starttime).seconds\n",
    "#         print(\"move model to gpu takes: \", time_cost, \"seconds\")\n",
    "        \n",
    "    wk_writer = SummaryWriter(\"ssp_resnet_runs/wk_\"+str(worker_index))\n",
    "    \n",
    "    for batch_idx,(data,target) in enumerate(train_loader):\n",
    "        if cuda:\n",
    "            starttime = datetime.datetime.now()\n",
    "\n",
    "            data,target = data.cuda(),target.cuda()\n",
    "            mid = datetime.datetime.now()\n",
    "#             print(\"move data to gpu takes: \", (mid - starttime).seconds, \"seconds\")\n",
    "            model.cuda()\n",
    "            endtime = datetime.datetime.now()\n",
    "            time_cost = (endtime - starttime).seconds\n",
    "#             print(\"move model to gpu takes: \", time_cost, \"seconds\")\n",
    "            \n",
    "        while(local_step - ray.get(ps.get_stalness.remote()) > stale_limit):\n",
    "            print(worker_index,\" works too fast\")\n",
    "            sleep(1)\n",
    "        # Get the current weights from the parameter server.\n",
    "#         print(\"the \",worker_index,\" pulls wei from ps.\")\n",
    "        init_wei = ray.get(ps.pull_weights.remote())\n",
    "        model.load_state_dict(init_wei)\n",
    "#         print(\"the \",worker_index,\" loaded the latest wei from ps.\")\n",
    "        # Compute an update and push it to the parameter server.        \n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "#         print(worker_index,' is generating output')\n",
    "        output = model(data)\n",
    "#         print(worker_index,' generated output done and going to calculate loss')\n",
    "        loss = F.cross_entropy(output,target)\n",
    "#         print(worker_index,' calculated loss and going to bp')\n",
    "        loss.backward()\n",
    "#         print(worker_index,' bp done')\n",
    "        \n",
    "        starttime = datetime.datetime.now()\n",
    "        model.cpu()\n",
    "        endtime = datetime.datetime.now()\n",
    "        time_cost = (endtime - starttime).seconds\n",
    "#         print(\"move model to cpu takes: \", time_cost, \"seconds\")\n",
    "\n",
    "\n",
    "        grad = [p.grad for p in model.parameters()]\n",
    "#         print(worker_index,' got the grad list')\n",
    "        local_step += 1\n",
    "        ps.apply_gradients.remote(grad,worker_index)\n",
    "#         print(worker_index,' sended the grad to ps and going to move next step')\n",
    "        optimizer.step()\n",
    "        wk_writer.add_scalar(\"Loss/worker_train\",loss,local_step)\n",
    "#         print(\"the \",worker_index,\" has finished its \",local_step,\" update\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_worker = 2\n",
    "stalness_table = [0] * num_worker\n",
    "stalness_limit = 4\n",
    "\n",
    "ps = ParameterServer.remote(lr,num_worker,stalness_limit,test_loader,None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_tasks = [worker_task.remote(ps,i,stalness_limit,train_loader[i],lr,momentum,weight_decay) \n",
    "                for i in range(num_worker)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filepath):\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(os.path.join(filepath, 'checkpoint.pth.tar'), os.path.join(filepath, 'model_best.pth.tar'))\n",
    "\n",
    "import datetime\n",
    "\n",
    "while True:\n",
    "    wei = ray.get(ps.pull_weights.remote())\n",
    "    save_checkpoint(wei,False,ckpt_ssp_resnet)\n",
    "    print(\"saved ckpt at: \", )\n",
    "    print(datetime.datetime.now())\n",
    "    time.sleep(600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
